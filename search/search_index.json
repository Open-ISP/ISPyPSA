{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":"<p>ISPyPSA is a Python package for building capacity expansion and operational models for the Australian National Electricity Market. The methodology is based on the Australian Energy Market Operator's (AEMO) Integrated System Plan (ISP). ISPyPSA converts AEMO's inputs and assumptions data into a format compatible with the PyPSA software, which is then used to run optimal capacity expansion and system operation.</p>"},{"location":"#beta-release-v010-beta","title":"Beta Release: v0.1.0-beta","text":"<p>Important</p> <p>ISPyPSA is at beta release! What does that mean?</p> <p>This version has been released primarily to support engagement and testing as the OpenISP team works towards a stable v1.0.0 release in 2026. For now, ISPyPSA lacks a few features and requires more testing before it will be ready for general use. However, please use, test, and explore ISPyPSA. Extensive data handling and model formulation functionality has already been built out and may be helpful to advanced users. We welcome all enquiries and feedback, please reach us at dylan.mcconnell@unsw.edu.au.</p>"},{"location":"api/","title":"ISPyPSA API","text":"<p>The ISPyPSA API is the set of Python functions used to carry out the ISPyPSA modelling workflow. When the CLI is used the API runs in the background to carry out the workflow.</p> <p>Advanced users wanting more control or flexibility from the modelling workflow might want to use ISPyPSA directly through the API. An example of how the default ISPyPSA workflow is implemented using the API is provided below. We suggest API users start by running and understanding the default workflow and then adapting it for their use case.</p> <p>Each API function is also documented individually after the workflow example.</p>"},{"location":"api/#api-default-workflow","title":"API default workflow","text":"<p>Below is an example of the default ISPyPSA workflow implemented using the Python API. This is the same workflow which the CLI follows. To use this workflow you simply need to edit the code to point at an ispypsa_config.yaml file of your choice, then run the Python script.</p> <pre><code>from pathlib import Path\n\nfrom ispypsa.config import load_config\nfrom ispypsa.data_fetch import read_csvs, write_csvs\nfrom ispypsa.iasr_table_caching import build_local_cache\nfrom ispypsa.logging import configure_logging\nfrom ispypsa.plotting import (\n    create_plot_suite,\n    generate_results_website,\n    save_plots,\n)\nfrom ispypsa.pypsa_build import (\n    build_pypsa_network,\n    save_pypsa_network,\n    update_network_timeseries,\n)\nfrom ispypsa.results import (\n    extract_regions_and_zones_mapping,\n    extract_tabular_results,\n)\nfrom ispypsa.templater import (\n    create_ispypsa_inputs_template,\n    load_manually_extracted_tables,\n)\nfrom ispypsa.translator import (\n    create_pypsa_friendly_inputs,\n    create_pypsa_friendly_timeseries_inputs,\n)\n\n# Load model config.\nconfig_path = Path(\"ispypsa_config.yaml\")\nconfig = load_config(config_path)\n\n# Load base paths from config\nparsed_workbook_cache = Path(config.paths.parsed_workbook_cache)\nparsed_traces_directory = (\n    Path(config.paths.parsed_traces_directory) / f\"isp_{config.trace_data.dataset_year}\"\n)\nworkbook_path = Path(config.paths.workbook_path)\nrun_directory = Path(config.paths.run_directory)\n\n# Construct full paths from base paths\nispypsa_input_tables_directory = (\n    run_directory / config.paths.ispypsa_run_name / \"ispypsa_inputs\"\n)\npypsa_friendly_inputs_location = (\n    run_directory / config.paths.ispypsa_run_name / \"pypsa_friendly\"\n)\ncapacity_expansion_timeseries_location = (\n    pypsa_friendly_inputs_location / \"capacity_expansion_timeseries\"\n)\noperational_timeseries_location = (\n    pypsa_friendly_inputs_location / \"operational_timeseries\"\n)\npypsa_outputs_directory = run_directory / config.paths.ispypsa_run_name / \"outputs\"\ncapacity_expansion_tabular_results_directory = (\n    pypsa_outputs_directory / \"capacity_expansion_tables\"\n)\ncapacity_expansion_plot_results_directory = (\n    pypsa_outputs_directory / \"capacity_expansion_plots\"\n)\noperational_tabular_results_directory = pypsa_outputs_directory / \"operational_tables\"\noperational_plot_results_directory = pypsa_outputs_directory / \"operational_plots\"\n\n# Create output directories if they don't exist\nparsed_workbook_cache.mkdir(parents=True, exist_ok=True)\nispypsa_input_tables_directory.mkdir(parents=True, exist_ok=True)\npypsa_friendly_inputs_location.mkdir(parents=True, exist_ok=True)\ncapacity_expansion_timeseries_location.mkdir(parents=True, exist_ok=True)\noperational_timeseries_location.mkdir(parents=True, exist_ok=True)\npypsa_outputs_directory.mkdir(parents=True, exist_ok=True)\ncapacity_expansion_tabular_results_directory.mkdir(parents=True, exist_ok=True)\ncapacity_expansion_plot_results_directory.mkdir(parents=True, exist_ok=True)\noperational_tabular_results_directory.mkdir(parents=True, exist_ok=True)\noperational_plot_results_directory.mkdir(parents=True, exist_ok=True)\n\nconfigure_logging()\n\n# Build the local cache from the workbook\nbuild_local_cache(parsed_workbook_cache, workbook_path, config.iasr_workbook_version)\n\n# Load ISP IASR data tables.\niasr_tables = read_csvs(parsed_workbook_cache)\nmanually_extracted_tables = load_manually_extracted_tables(config.iasr_workbook_version)\n\n# Create ISPyPSA inputs from IASR tables.\nispypsa_tables = create_ispypsa_inputs_template(\n    config.scenario,\n    config.network.nodes.regional_granularity,\n    iasr_tables,\n    manually_extracted_tables,\n    config.filter_by_nem_regions,\n    config.filter_by_isp_sub_regions,\n)\nwrite_csvs(ispypsa_tables, ispypsa_input_tables_directory)\n\n# Suggested stage of user interaction:\n# At this stage of the workflow the user can modify ispypsa input files, either\n# manually or programmatically, to run alternative scenarios using the template\n# generated from the chosen ISP scenario.\n\n# Translate ISPyPSA format to a PyPSA friendly format.\npypsa_friendly_input_tables = create_pypsa_friendly_inputs(config, ispypsa_tables)\n\n# Create timeseries inputs and snapshots\npypsa_friendly_input_tables[\"snapshots\"] = create_pypsa_friendly_timeseries_inputs(\n    config,\n    \"capacity_expansion\",\n    ispypsa_tables,\n    pypsa_friendly_input_tables[\"generators\"],\n    parsed_traces_directory,\n    capacity_expansion_timeseries_location,\n)\n\nwrite_csvs(pypsa_friendly_input_tables, pypsa_friendly_inputs_location)\n\n# Build a PyPSA network object.\nnetwork = build_pypsa_network(\n    pypsa_friendly_input_tables,\n    capacity_expansion_timeseries_location,\n)\n\n# Solve for least cost operation/expansion\n# Never use network.optimize() as this will remove custom constraints.\nnetwork.optimize.solve_model(solver_name=config.solver)\n\n# Save capacity expansion results\nsave_pypsa_network(network, pypsa_outputs_directory, \"capacity_expansion\")\ncapacity_expansion_results = extract_tabular_results(network, ispypsa_tables)\ncapacity_expansion_results[\"regions_and_zones_mapping\"] = (\n    extract_regions_and_zones_mapping(ispypsa_tables)\n)\nwrite_csvs(capacity_expansion_results, capacity_expansion_tabular_results_directory)\n\n# Create and save capacity expansion plots\ncapacity_expansion_plots = create_plot_suite(capacity_expansion_results)\nsave_plots(capacity_expansion_plots, capacity_expansion_plot_results_directory)\n\n# Generate capacity expansion results website\ngenerate_results_website(\n    capacity_expansion_plots,\n    capacity_expansion_plot_results_directory,\n    pypsa_outputs_directory,\n    site_name=f\"{config.paths.ispypsa_run_name}\",\n    output_filename=\"capacity_expansion_results_viewer.html\",\n    subtitle=\"Capacity Expansion Analysis\",\n    regions_and_zones_mapping=capacity_expansion_results[\"regions_and_zones_mapping\"],\n)\n\n# Operational modelling extension\noperational_snapshots = create_pypsa_friendly_timeseries_inputs(\n    config,\n    \"operational\",\n    ispypsa_tables,\n    pypsa_friendly_input_tables[\"generators\"],\n    parsed_traces_directory,\n    operational_timeseries_location,\n)\n\nwrite_csvs(\n    {\"operational_snapshots\": operational_snapshots}, pypsa_friendly_inputs_location\n)\n\nupdate_network_timeseries(\n    network,\n    pypsa_friendly_input_tables,\n    operational_snapshots,\n    operational_timeseries_location,\n)\n\nnetwork.optimize.fix_optimal_capacities()\n\n# Never use network.optimize() as this will remove custom constraints.\nnetwork.optimize.optimize_with_rolling_horizon(\n    horizon=config.temporal.operational.horizon,\n    overlap=config.temporal.operational.overlap,\n)\n\nsave_pypsa_network(network, pypsa_outputs_directory, \"operational\")\n\n# Extract and save operational results\noperational_results = extract_tabular_results(network, ispypsa_tables)\noperational_results[\"regions_and_zones_mapping\"] = extract_regions_and_zones_mapping(\n    ispypsa_tables\n)\nwrite_csvs(operational_results, operational_tabular_results_directory)\n\n# Create and save operational plots\noperational_plots = create_plot_suite(operational_results)\nsave_plots(operational_plots, operational_plot_results_directory)\n\n# Generate operational results website\ngenerate_results_website(\n    operational_plots,\n    operational_plot_results_directory,\n    pypsa_outputs_directory,\n    site_name=f\"{config.paths.ispypsa_run_name}\",\n    output_filename=\"operational_results_viewer.html\",\n    subtitle=\"Operational Analysis\",\n    regions_and_zones_mapping=operational_results[\"regions_and_zones_mapping\"],\n)\n</code></pre>"},{"location":"api/#configuration-logging","title":"Configuration &amp; Logging","text":""},{"location":"api/#ispypsa.config.load_config","title":"ispypsa.config.load_config","text":"<pre><code>load_config(config_path: str | Path) -&gt; ModelConfig\n</code></pre> <p>Load and validate configuration from a YAML file.</p> <p>Examples:</p> <p>Perform required imports.</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ispypsa.config import load_config\n</code></pre> <p>Load the configuration from a YAML file.</p> <pre><code>&gt;&gt;&gt; config = load_config(Path(\"ispypsa_config.yaml\"))\n</code></pre> <p>Access configuration values.</p> <pre><code>&gt;&gt;&gt; config.scenario\n'Step Change'\n&gt;&gt;&gt; config.network.nodes.regional_granularity\n'sub_regions'\n</code></pre> <p>Parameters:</p> <ul> <li> <code>config_path</code>               (<code>str | Path</code>)           \u2013            <p>Path to the YAML configuration file</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ModelConfig</code> (              <code>ModelConfig</code> )          \u2013            <p>Validated configuration object</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValidationError</code>             \u2013            <p>If the configuration is invalid</p> </li> <li> <code>FileNotFoundError</code>             \u2013            <p>If the config file doesn't exist</p> </li> <li> <code>YAMLError</code>             \u2013            <p>If the YAML is malformed</p> </li> </ul> Source code in <code>src\\ispypsa\\config\\loader.py</code> <pre><code>def load_config(config_path: str | Path) -&gt; ModelConfig:\n    \"\"\"\n    Load and validate configuration from a YAML file.\n\n    Examples:\n        Perform required imports.\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; from ispypsa.config import load_config\n\n        Load the configuration from a YAML file.\n        &gt;&gt;&gt; config = load_config(Path(\"ispypsa_config.yaml\"))\n\n        Access configuration values.\n        &gt;&gt;&gt; config.scenario\n        'Step Change'\n        &gt;&gt;&gt; config.network.nodes.regional_granularity\n        'sub_regions'\n\n    Args:\n        config_path: Path to the YAML configuration file\n\n    Returns:\n        ModelConfig: Validated configuration object\n\n    Raises:\n        ValidationError: If the configuration is invalid\n        FileNotFoundError: If the config file doesn't exist\n        yaml.YAMLError: If the YAML is malformed\n    \"\"\"\n    with open(config_path) as f:\n        config_dict = yaml.safe_load(f)\n\n    return ModelConfig(**config_dict)\n</code></pre>"},{"location":"api/#ispypsa.logging.configure_logging","title":"ispypsa.logging.configure_logging","text":"<pre><code>configure_logging(console: bool = True, console_level: int = logging.WARNING, file: bool = True, file_level: int = logging.INFO, log_file: str = 'ISPyPSA.log') -&gt; None\n</code></pre> <p>Configures ISPyPSA logging</p> <p>Examples:</p> <p>Perform required imports.</p> <pre><code>&gt;&gt;&gt; import logging\n&gt;&gt;&gt; from ispypsa.logging import configure_logging\n</code></pre> <p>Configure logging with default settings (console warnings, file info).</p> <pre><code>&gt;&gt;&gt; configure_logging()\n</code></pre> <p>Configure logging with custom settings.</p> <pre><code>&gt;&gt;&gt; configure_logging(\n...     console=True,\n...     console_level=logging.INFO,\n...     file=True,\n...     file_level=logging.DEBUG,\n...     log_file=\"my_run.log\"\n... )\n</code></pre> <p>Disable file logging.</p> <pre><code>&gt;&gt;&gt; configure_logging(file=False)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>console</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to log to the console. Defaults to True.</p> </li> <li> <code>console_level</code>               (<code>int</code>, default:                   <code>WARNING</code> )           \u2013            <p>Level of the console logging. Defaults to logging.WARNING.</p> </li> <li> <code>file</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to log to a log file. Defaults to True.</p> </li> <li> <code>file_level</code>               (<code>int</code>, default:                   <code>INFO</code> )           \u2013            <p>Level of the file logging. Defaults to logging.INFO.</p> </li> <li> <code>log_file</code>               (<code>str</code>, default:                   <code>'ISPyPSA.log'</code> )           \u2013            <p>Name of the logging file. Defaults to \"ISPyPSA.log\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None</p> </li> </ul> Source code in <code>src\\ispypsa\\logging.py</code> <pre><code>def configure_logging(\n    console: bool = True,\n    console_level: int = logging.WARNING,\n    file: bool = True,\n    file_level: int = logging.INFO,\n    log_file: str = \"ISPyPSA.log\",\n) -&gt; None:\n    \"\"\"Configures ISPyPSA logging\n\n    Examples:\n        Perform required imports.\n        &gt;&gt;&gt; import logging\n        &gt;&gt;&gt; from ispypsa.logging import configure_logging\n\n        Configure logging with default settings (console warnings, file info).\n        &gt;&gt;&gt; configure_logging()\n\n        Configure logging with custom settings.\n        &gt;&gt;&gt; configure_logging(\n        ...     console=True,\n        ...     console_level=logging.INFO,\n        ...     file=True,\n        ...     file_level=logging.DEBUG,\n        ...     log_file=\"my_run.log\"\n        ... )\n\n        Disable file logging.\n        &gt;&gt;&gt; configure_logging(file=False)\n\n    Args:\n        console: Whether to log to the console. Defaults to True.\n        console_level: Level of the console logging. Defaults to logging.WARNING.\n        file: Whether to log to a log file. Defaults to True.\n        file_level: Level of the file logging. Defaults to logging.INFO.\n        log_file: Name of the logging file. Defaults to \"ISPyPSA.log\".\n\n    Returns:\n        None\n    \"\"\"\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    handlers = []\n    if console:\n        console_handler = logging.StreamHandler(stream=sys.stdout)\n        console_handler.setLevel(console_level)\n        console_formatter = logging.Formatter(\"%(levelname)s: %(message)s\")\n        console_handler.setFormatter(console_formatter)\n        handlers.append(console_handler)\n    if file:\n        file_handler = logging.FileHandler(log_file)\n        file_handler.setLevel(file_level)\n        file_formatter = logging.Formatter(\"%(asctime)s - %(levelname)s: %(message)s\")\n        file_handler.setFormatter(file_formatter)\n        handlers.append(file_handler)\n    if not handlers:\n        handlers.append(logging.NullHandler())\n    logging.basicConfig(\n        level=logging.INFO,\n        format=\"[%(asctime)s] %(levelname)s: %(message)s\",\n        handlers=handlers,\n    )\n    configure_dependency_logger(\"pypsa\", logging.INFO)\n</code></pre>"},{"location":"api/#data-fetching-caching","title":"Data Fetching &amp; Caching","text":""},{"location":"api/#ispypsa.iasr_table_caching.build_local_cache","title":"ispypsa.iasr_table_caching.build_local_cache","text":"<pre><code>build_local_cache(cache_path: Path | str, workbook_path: Path | str, iasr_workbook_version: str) -&gt; None\n</code></pre> <p>Uses <code>isp-workbook-parser</code> to build a local cache of parsed workbook CSVs</p> <p>Examples:</p> <p>Perform required imports.</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ispypsa.iasr_table_caching import build_local_cache\n</code></pre> <p>Build the local cache of parsed workbook CSVs.</p> <pre><code>&gt;&gt;&gt; build_local_cache(\n...     cache_path=Path(\"parsed_workbook_cache\"),\n...     workbook_path=Path(\"path/to/ISP_Workbook.xlsx\"),\n...     iasr_workbook_version=\"6.0\"\n... )\n</code></pre> <p>Parameters:</p> <ul> <li> <code>cache_path</code>               (<code>Path | str</code>)           \u2013            <p>Path that should be created for the local cache</p> </li> <li> <code>workbook_path</code>               (<code>Path | str</code>)           \u2013            <p>Path to an ISP Assumptions Workbook that is supported by <code>isp-workbook-parser</code></p> </li> <li> <code>iasr_workbook_version</code>               (<code>str</code>)           \u2013            <p>str specifying the version of the work being used.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None</p> </li> </ul> Source code in <code>src\\ispypsa\\iasr_table_caching\\local_cache.py</code> <pre><code>def build_local_cache(\n    cache_path: Path | str, workbook_path: Path | str, iasr_workbook_version: str\n) -&gt; None:\n    \"\"\"Uses `isp-workbook-parser` to build a local cache of parsed workbook CSVs\n\n    Examples:\n        Perform required imports.\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; from ispypsa.iasr_table_caching import build_local_cache\n\n        Build the local cache of parsed workbook CSVs.\n        &gt;&gt;&gt; build_local_cache(\n        ...     cache_path=Path(\"parsed_workbook_cache\"),\n        ...     workbook_path=Path(\"path/to/ISP_Workbook.xlsx\"),\n        ...     iasr_workbook_version=\"6.0\"\n        ... )\n\n    Args:\n        cache_path: Path that should be created for the local cache\n        workbook_path: Path to an ISP Assumptions Workbook that is supported by\n            `isp-workbook-parser`\n        iasr_workbook_version: str specifying the version of the work being used.\n\n    Returns:\n        None\n    \"\"\"\n    workbook = Parser(Path(workbook_path))\n    if workbook.workbook_version != iasr_workbook_version:\n        raise ValueError(\n            \"The IASR workbook provided does not match the version \"\n            \"specified in the config.\"\n        )\n    tables_to_get = REQUIRED_TABLES\n    workbook.save_tables(cache_path, tables=tables_to_get)\n    return None\n</code></pre>"},{"location":"api/#ispypsa.data_fetch.read_csvs","title":"ispypsa.data_fetch.read_csvs","text":"<pre><code>read_csvs(directory: Path | str) -&gt; dict[str:(pd.DataFrame)]\n</code></pre> <p>Read all the CSVs in a directory into a dictionary with filenames (without csv extension) as keys.</p> <p>Examples:</p> <p>Perform required imports.</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ispypsa.data_fetch import read_csvs\n</code></pre> <p>Read CSVs from a directory containing parsed workbook tables.</p> <pre><code>&gt;&gt;&gt; iasr_tables = read_csvs(Path(\"parsed_workbook_cache\"))\n</code></pre> <p>Read CSVs from a directory containing ISPyPSA input tables.</p> <pre><code>&gt;&gt;&gt; ispypsa_tables = read_csvs(Path(\"ispypsa_inputs\"))\n</code></pre> <p>Access a specific table from the dictionary.</p> <pre><code>&gt;&gt;&gt; generators = iasr_tables[\"existing_generators_summary\"]\n</code></pre> <p>Parameters:</p> <ul> <li> <code>directory</code>               (<code>Path | str</code>)           \u2013            <p>Path to directory to read CSVs from.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str:(DataFrame)]</code>           \u2013            <p>dict[str, pd.DataFrame]: Dictionary with filenames (without .csv extension)</p> </li> <li> <code>dict[str:(DataFrame)]</code>           \u2013            <p>as keys and DataFrames as values.</p> </li> </ul> Source code in <code>src\\ispypsa\\data_fetch\\csv_read_write.py</code> <pre><code>def read_csvs(directory: Path | str) -&gt; dict[str : pd.DataFrame]:\n    \"\"\"Read all the CSVs in a directory into a dictionary with filenames (without csv\n    extension) as keys.\n\n    Examples:\n        Perform required imports.\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; from ispypsa.data_fetch import read_csvs\n\n        Read CSVs from a directory containing parsed workbook tables.\n        &gt;&gt;&gt; iasr_tables = read_csvs(Path(\"parsed_workbook_cache\"))\n\n        Read CSVs from a directory containing ISPyPSA input tables.\n        &gt;&gt;&gt; ispypsa_tables = read_csvs(Path(\"ispypsa_inputs\"))\n\n        Access a specific table from the dictionary.\n        &gt;&gt;&gt; generators = iasr_tables[\"existing_generators_summary\"]\n\n    Args:\n        directory: Path to directory to read CSVs from.\n\n    Returns:\n        dict[str, pd.DataFrame]: Dictionary with filenames (without .csv extension)\n        as keys and DataFrames as values.\n    \"\"\"\n    files = Path(directory).glob(\"*.csv\")\n    return {file.name[:-4]: pd.read_csv(file) for file in files}\n</code></pre>"},{"location":"api/#ispypsa.data_fetch.write_csvs","title":"ispypsa.data_fetch.write_csvs","text":"<pre><code>write_csvs(data_dict: dict[str:(DataFrame)], directory: Path | str)\n</code></pre> <p>Write all pd.DataFrames in a dictionary with filenames as keys (without csv extension) to CSVs.</p> <p>Examples:</p> <p>Perform required imports.</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ispypsa.data_fetch import write_csvs\n</code></pre> <p>Write ISPyPSA input tables to a directory.</p> <pre><code>&gt;&gt;&gt; write_csvs(ispypsa_tables, Path(\"ispypsa_inputs\"))\n</code></pre> <p>Write PyPSA-friendly tables to a directory.</p> <pre><code>&gt;&gt;&gt; write_csvs(pypsa_friendly_tables, Path(\"pypsa_friendly_inputs\"))\n</code></pre> <p>Write model results to a directory.</p> <pre><code>&gt;&gt;&gt; write_csvs(results, Path(\"outputs/results\"))\n</code></pre> <p>Parameters:</p> <ul> <li> <code>data_dict</code>               (<code>dict[str:(DataFrame)]</code>)           \u2013            <p>Dictionary of pd.DataFrames to write to csv files.</p> </li> <li> <code>directory</code>               (<code>Path | str</code>)           \u2013            <p>Path to directory to save CSVs to.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>None</p> </li> </ul> Source code in <code>src\\ispypsa\\data_fetch\\csv_read_write.py</code> <pre><code>def write_csvs(data_dict: dict[str : pd.DataFrame], directory: Path | str):\n    \"\"\"Write all pd.DataFrames in a dictionary with filenames as keys (without csv extension)\n    to CSVs.\n\n    Examples:\n        Perform required imports.\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; from ispypsa.data_fetch import write_csvs\n\n        Write ISPyPSA input tables to a directory.\n        &gt;&gt;&gt; write_csvs(ispypsa_tables, Path(\"ispypsa_inputs\"))\n\n        Write PyPSA-friendly tables to a directory.\n        &gt;&gt;&gt; write_csvs(pypsa_friendly_tables, Path(\"pypsa_friendly_inputs\"))\n\n        Write model results to a directory.\n        &gt;&gt;&gt; write_csvs(results, Path(\"outputs/results\"))\n\n    Args:\n        data_dict: Dictionary of pd.DataFrames to write to csv files.\n        directory: Path to directory to save CSVs to.\n\n    Returns:\n        None\n    \"\"\"\n    for file_name, data in data_dict.items():\n        save_path = Path(directory) / Path(f\"{file_name}.csv\")\n        save_path.parent.mkdir(parents=True, exist_ok=True)\n        # set index=False to avoid adding \"Unnamed\" cols if/when reading from these csvs later\n        data.to_csv(save_path, index=False)\n</code></pre>"},{"location":"api/#ispypsa.data_fetch.fetch_workbook","title":"ispypsa.data_fetch.fetch_workbook","text":"<pre><code>fetch_workbook(workbook_version: str, save_path: Path | str) -&gt; None\n</code></pre> <p>Download ISP workbook file.</p> <p>Downloads the ISP workbook for the specified version from the manifest to the specified file path.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fetch_workbook(\"6.0\", \"data/workbooks/isp_6.0.xlsx\")\n# Downloads ISP 6.0 workbook to data/workbooks/isp_6.0.xlsx\n</code></pre> <p>Parameters:</p> <ul> <li> <code>workbook_version </code>           \u2013            <p>str Version string (e.g., \"6.0\")</p> </li> <li> <code>save_path </code>           \u2013            <p>Path | str Full path where the workbook file should be saved (e.g., \"data/workbooks/6.0.xlsx\")</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If the manifest file does not exist</p> </li> <li> <code>HTTPError</code>             \u2013            <p>If the download fails</p> </li> </ul> Source code in <code>src\\ispypsa\\data_fetch\\download.py</code> <pre><code>def fetch_workbook(\n    workbook_version: str,\n    save_path: Path | str,\n) -&gt; None:\n    \"\"\"Download ISP workbook file.\n\n    Downloads the ISP workbook for the specified version from the manifest\n    to the specified file path.\n\n    Examples:\n        &gt;&gt;&gt; fetch_workbook(\"6.0\", \"data/workbooks/isp_6.0.xlsx\")\n        # Downloads ISP 6.0 workbook to data/workbooks/isp_6.0.xlsx\n\n    Args:\n        workbook_version : str\n            Version string (e.g., \"6.0\")\n        save_path : Path | str\n            Full path where the workbook file should be saved\n            (e.g., \"data/workbooks/6.0.xlsx\")\n\n    Returns:\n        None\n\n    Raises:\n        FileNotFoundError: If the manifest file does not exist\n        requests.HTTPError: If the download fails\n    \"\"\"\n    # Construct manifest path\n    manifest_path = (\n        Path(__file__).parent / \"manifests\" / \"workbooks\" / f\"{workbook_version}.txt\"\n    )\n\n    if not manifest_path.exists():\n        raise FileNotFoundError(f\"Manifest file not found: {manifest_path}\")\n\n    # Read URL from manifest (should be single URL)\n    with open(manifest_path) as f:\n        urls = [line.strip() for line in f if line.strip()]\n\n    url = urls[0]\n    save_path = Path(save_path)\n\n    # Create parent directories\n    save_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Download file\n    response = requests.get(url, stream=True)\n    response.raise_for_status()\n\n    # Get file size if available\n    total_size = int(response.headers.get(\"content-length\", 0))\n\n    # Write file with progress bar\n    with (\n        open(save_path, \"wb\") as f,\n        tqdm(\n            total=total_size,\n            unit=\"B\",\n            unit_scale=True,\n            unit_divisor=1024,\n            desc=f\"Downloading {save_path.name}\",\n        ) as pbar,\n    ):\n        for chunk in response.iter_content(chunk_size=8192):\n            f.write(chunk)\n            pbar.update(len(chunk))\n</code></pre>"},{"location":"api/#templating-ispypsa-input-creation","title":"Templating (ISPyPSA Input Creation)","text":""},{"location":"api/#ispypsa.templater.create_ispypsa_inputs_template","title":"ispypsa.templater.create_ispypsa_inputs_template","text":"<pre><code>create_ispypsa_inputs_template(scenario: str, regional_granularity: str, iasr_tables: dict[str:(DataFrame)], manually_extracted_tables: dict[str:(DataFrame)], filter_to_nem_regions: list[str] = None, filter_to_isp_sub_regions: list[str] = None) -&gt; dict[str:(pd.DataFrame)]\n</code></pre> <p>Creates a template set of <code>ISPyPSA</code> input tables.</p> <p>Examples:</p> <p>Perform required imports.</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ispypsa.config import load_config\n&gt;&gt;&gt; from ispypsa.data_fetch import read_csvs, write_csvs\n&gt;&gt;&gt; from ispypsa.templater import load_manually_extracted_tables\n&gt;&gt;&gt; from ispypsa.templater import create_ispypsa_inputs_template\n</code></pre> <p>Tables previously extracted from IASR workbook using isp_workbook_parser are loaded.</p> <pre><code>&gt;&gt;&gt; iasr_tables = read_csvs(Path(\"iasr_directory\"))\n</code></pre> <p>Some tables can't be handled by isp_workbook_parser so ISPyPSA ships with the missing data.</p> <pre><code>&gt;&gt;&gt; manually_extracted_tables = load_manually_extracted_tables(\"6.0\")\n</code></pre> <p>Now a template can be created by specifying the ISP scenario to use and the spacial granularity of model.</p> <pre><code>&gt;&gt;&gt; ispypsa_inputs_template = create_ispypsa_inputs_template(\n... scenario=\"Step Change\",\n... regional_granularity=\"sub_regions\",\n... iasr_tables=iasr_tables,\n... manually_extracted_tables=manually_extracted_tables\n... )\n</code></pre> <p>Write the template tables to a directory as CSVs.</p> <pre><code>&gt;&gt;&gt; write_csvs(ispypsa_inputs_template, Path(\"ispypsa_inputs\"))\n</code></pre> <p>Parameters:</p> <ul> <li> <code>scenario</code>               (<code>str</code>)           \u2013            <p>ISP scenario to generate template inputs based on.</p> </li> <li> <code>regional_granularity</code>               (<code>str</code>)           \u2013            <p>the spatial granularity of the model template, \"sub_regions\", \"nem_regions\", or \"single_region\".</p> </li> <li> <code>iasr_tables</code>               (<code>dict[str:(DataFrame)]</code>)           \u2013            <p>dictionary of dataframes providing the IASR input tables extracted using the <code>isp_workbook_parser</code>.</p> </li> <li> <code>manually_extracted_tables</code>               (<code>dict[str:(DataFrame)]</code>)           \u2013            <p>dictionary of dataframes providing additional IASR tables that can't be parsed using <code>isp_workbook_parser</code></p> </li> <li> <code>filter_to_nem_regions</code>               (<code>list[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of NEM region IDs (e.g., ['NSW', 'VIC']) to filter the template to. Cannot be specified together with filter_to_isp_sub_regions.</p> </li> <li> <code>filter_to_isp_sub_regions</code>               (<code>list[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of ISP sub-region IDs (e.g., ['CNSW', 'VIC', 'TAS']) to filter the template to. Cannot be specified together with filter_to_nem_regions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str:(DataFrame)]</code>           \u2013            <p>dictionary of dataframes in the <code>ISPyPSA</code> format</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If both filter_to_nem_regions and filter_to_isp_sub_regions are provided</p> </li> </ul> Source code in <code>src\\ispypsa\\templater\\create_template.py</code> <pre><code>def create_ispypsa_inputs_template(\n    scenario: str,\n    regional_granularity: str,\n    iasr_tables: dict[str : pd.DataFrame],\n    manually_extracted_tables: dict[str : pd.DataFrame],\n    filter_to_nem_regions: list[str] = None,\n    filter_to_isp_sub_regions: list[str] = None,\n) -&gt; dict[str : pd.DataFrame]:\n    \"\"\"Creates a template set of [`ISPyPSA` input tables](tables/ispypsa.md).\n\n    Examples:\n        Perform required imports.\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; from ispypsa.config import load_config\n        &gt;&gt;&gt; from ispypsa.data_fetch import read_csvs, write_csvs\n        &gt;&gt;&gt; from ispypsa.templater import load_manually_extracted_tables\n        &gt;&gt;&gt; from ispypsa.templater import create_ispypsa_inputs_template\n\n        Tables previously extracted from IASR workbook using isp_workbook_parser are\n        loaded.\n        &gt;&gt;&gt; iasr_tables = read_csvs(Path(\"iasr_directory\"))\n\n        Some tables can't be handled by isp_workbook_parser so ISPyPSA ships with the\n        missing data.\n        &gt;&gt;&gt; manually_extracted_tables = load_manually_extracted_tables(\"6.0\")\n\n        Now a template can be created by specifying the ISP scenario to use and the\n        spacial granularity of model.\n        &gt;&gt;&gt; ispypsa_inputs_template = create_ispypsa_inputs_template(\n        ... scenario=\"Step Change\",\n        ... regional_granularity=\"sub_regions\",\n        ... iasr_tables=iasr_tables,\n        ... manually_extracted_tables=manually_extracted_tables\n        ... )\n\n        Write the template tables to a directory as CSVs.\n        &gt;&gt;&gt; write_csvs(ispypsa_inputs_template, Path(\"ispypsa_inputs\"))\n\n    Args:\n        scenario: ISP scenario to generate template inputs based on.\n        regional_granularity: the spatial granularity of the model template,\n            \"sub_regions\", \"nem_regions\", or \"single_region\".\n        iasr_tables: dictionary of dataframes providing the IASR input tables\n            extracted using the `isp_workbook_parser`.\n        manually_extracted_tables: dictionary of dataframes providing additional\n            IASR tables that can't be parsed using `isp_workbook_parser`\n        filter_to_nem_regions: Optional list of NEM region IDs (e.g., ['NSW', 'VIC'])\n            to filter the template to. Cannot be specified together with\n            filter_to_isp_sub_regions.\n        filter_to_isp_sub_regions: Optional list of ISP sub-region IDs\n            (e.g., ['CNSW', 'VIC', 'TAS']) to filter the template to. Cannot be\n            specified together with filter_to_nem_regions.\n\n    Returns:\n        dictionary of dataframes in the [`ISPyPSA` format](tables/ispypsa.md)\n\n    Raises:\n        ValueError: If both filter_to_nem_regions and filter_to_isp_sub_regions are provided\n    \"\"\"\n    # Validate filtering parameters\n    if filter_to_nem_regions is not None and filter_to_isp_sub_regions is not None:\n        raise ValueError(\n            \"Cannot specify both filter_to_nem_regions and filter_to_isp_sub_regions\"\n        )\n\n    template = {}\n\n    template.update(manually_extracted_tables)\n\n    if regional_granularity == \"sub_regions\":\n        template[\"sub_regions\"] = _template_sub_regions(\n            iasr_tables[\"sub_regional_reference_nodes\"], mapping_only=False\n        )\n\n        template[\"flow_paths\"] = _template_sub_regional_flow_paths(\n            iasr_tables[\"flow_path_transfer_capability\"]\n        )\n\n        template[\"flow_path_expansion_costs\"] = _template_sub_regional_flow_path_costs(\n            iasr_tables,\n            scenario,\n        )\n\n    elif regional_granularity == \"nem_regions\":\n        template[\"sub_regions\"] = _template_sub_regions(\n            iasr_tables[\"sub_regional_reference_nodes\"], mapping_only=True\n        )\n\n        template[\"nem_regions\"] = _template_regions(\n            iasr_tables[\"regional_reference_nodes\"]\n        )\n\n        template[\"flow_paths\"] = _template_regional_interconnectors(\n            iasr_tables[\"interconnector_transfer_capability\"]\n        )\n\n    else:\n        template[\"sub_regions\"] = _template_sub_regions(\n            iasr_tables[\"sub_regional_reference_nodes\"], mapping_only=True\n        )\n\n    template[\"renewable_energy_zones\"] = _template_rez_build_limits(\n        iasr_tables[\"initial_build_limits\"], scenario\n    )\n\n    possible_rez_or_constraint_names = list(\n        set(\n            list(template[\"renewable_energy_zones\"][\"rez_id\"])\n            + list(template[\"custom_constraints_rhs\"][\"constraint_id\"])\n        )\n    )\n\n    template[\"rez_transmission_expansion_costs\"] = _template_rez_transmission_costs(\n        iasr_tables,\n        scenario,\n        possible_rez_or_constraint_names,\n    )\n\n    template[\"ecaa_generators\"] = _template_ecaa_generators_static_properties(\n        iasr_tables\n    )\n\n    template[\"new_entrant_generators\"] = _template_new_generators_static_properties(\n        iasr_tables\n    )\n\n    ecaa_batteries, new_entrant_batteries = _template_battery_properties(iasr_tables)\n    template[\"ecaa_batteries\"] = ecaa_batteries\n    template[\"new_entrant_batteries\"] = new_entrant_batteries\n\n    dynamic_generator_property_templates = _template_generator_dynamic_properties(\n        iasr_tables, scenario\n    )\n\n    template.update(dynamic_generator_property_templates)\n\n    energy_policy_targets = _template_energy_policy_targets(iasr_tables, scenario)\n\n    template.update(energy_policy_targets)\n\n    # Apply regional filtering if requested\n    if filter_to_nem_regions or filter_to_isp_sub_regions:\n        template = _filter_template(\n            template,\n            nem_regions=filter_to_nem_regions,\n            isp_sub_regions=filter_to_isp_sub_regions,\n        )\n\n    return template\n</code></pre>"},{"location":"api/#ispypsa.templater.load_manually_extracted_tables","title":"ispypsa.templater.load_manually_extracted_tables","text":"<pre><code>load_manually_extracted_tables(iasr_workbook_version: str) -&gt; dict[str:(pd.DataFrame)]\n</code></pre> <p>Retrieves the manually extracted template files for the IASR workbook version.</p> <p>Some tables can't be handled by <code>isp-workbook-parser</code> so ISPyPSA ships with the missing data pre-extracted for each supported workbook version.</p> <p>Examples:</p> <p>Perform required imports.</p> <pre><code>&gt;&gt;&gt; from ispypsa.templater import load_manually_extracted_tables\n</code></pre> <p>Load the manually extracted tables for the workbook version.</p> <pre><code>&gt;&gt;&gt; manually_extracted_tables = load_manually_extracted_tables(\"6.0\")\n</code></pre> <p>Access a specific table from the dictionary.</p> <pre><code>&gt;&gt;&gt; custom_constraints_rhs = manually_extracted_tables[\"custom_constraints_rhs\"]\n</code></pre> <p>Parameters:</p> <ul> <li> <code>iasr_workbook_version</code>               (<code>str</code>)           \u2013            <p>str specifying which version of the workbook is being used to create the template.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str:(DataFrame)]</code>           \u2013            <p>dict[str: <code>pd.DataFrame</code>]</p> </li> </ul> Source code in <code>src\\ispypsa\\templater\\manual_tables.py</code> <pre><code>def load_manually_extracted_tables(\n    iasr_workbook_version: str,\n) -&gt; dict[str : pd.DataFrame]:\n    \"\"\"Retrieves the manually extracted template files for the IASR workbook version.\n\n    Some tables can't be handled by `isp-workbook-parser` so ISPyPSA ships with the\n    missing data pre-extracted for each supported workbook version.\n\n    Examples:\n        Perform required imports.\n        &gt;&gt;&gt; from ispypsa.templater import load_manually_extracted_tables\n\n        Load the manually extracted tables for the workbook version.\n        &gt;&gt;&gt; manually_extracted_tables = load_manually_extracted_tables(\"6.0\")\n\n        Access a specific table from the dictionary.\n        &gt;&gt;&gt; custom_constraints_rhs = manually_extracted_tables[\"custom_constraints_rhs\"]\n\n    Args:\n        iasr_workbook_version: str specifying which version of the workbook is being\n            used to create the template.\n\n    Returns:\n        dict[str: `pd.DataFrame`]\n    \"\"\"\n    path_to_tables = (\n        Path(__file__).parent\n        / Path(\"manually_extracted_template_tables\")\n        / Path(iasr_workbook_version)\n    )\n    csv_files = path_to_tables.glob(\"*.csv\")\n    df_files = {}\n    for file in csv_files:\n        df_files[file.name.replace(\".csv\", \"\")] = pd.read_csv(file)\n    return df_files\n</code></pre>"},{"location":"api/#translation-pypsa-friendly-format","title":"Translation (PyPSA-Friendly Format)","text":""},{"location":"api/#ispypsa.translator.create_pypsa_friendly_inputs","title":"ispypsa.translator.create_pypsa_friendly_inputs","text":"<pre><code>create_pypsa_friendly_inputs(config: ModelConfig, ispypsa_tables: dict[str, DataFrame]) -&gt; dict[str, pd.DataFrame]\n</code></pre> <p>Creates a set of tables for defining a <code>PyPSA</code> network from a set <code>ISPyPSA</code> tables.</p> <p>Examples:</p> <p>Perform requried imports.</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ispypsa.config import load_config\n&gt;&gt;&gt; from ispypsa.data_fetch import read_csvs, write_csvs\n&gt;&gt;&gt; from ispypsa.translator import create_pypsa_friendly_inputs\n</code></pre> <p>Load ISPyPSA model config file and input tables.</p> <pre><code>&gt;&gt;&gt; config = load_config(Path(\"ispypsa_config.yaml\"))\n&gt;&gt;&gt; ispypsa_input_tables = read_csvs(Path(\"ispypsa_inputs_directory\"))\n</code></pre> <p>Make the PyPSA friendly inputs!</p> <pre><code>&gt;&gt;&gt; pypsa_friendly_inputs = create_pypsa_friendly_inputs(\n... config=config,\n... ispypsa_tables=ispypsa_input_tables\n... )\n</code></pre> <p>Write the resulting dataframes to CSVs.</p> <pre><code>&gt;&gt;&gt; write_csvs(pypsa_friendly_inputs, Path(\"pypsa_friendly_inputs\"))\n</code></pre> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>ModelConfig</code>)           \u2013            <p><code>ISPyPSA</code> <code>ispypsa.config.ModelConfig</code> object (add link to config docs).</p> </li> <li> <code>ispypsa_tables</code>               (<code>dict[str, DataFrame]</code>)           \u2013            <p>dictionary of dataframes providing the <code>ISPyPSA</code> input tables. (add link to ispypsa input tables docs).</p> </li> </ul> <p>dictionary of dataframes in the `PyPSA` friendly format. (add link to</p> <ul> <li> <code>dict[str, DataFrame]</code>           \u2013            <p>pypsa friendly format table docs)</p> </li> </ul> Source code in <code>src\\ispypsa\\translator\\create_pypsa_friendly.py</code> <pre><code>def create_pypsa_friendly_inputs(\n    config: ModelConfig, ispypsa_tables: dict[str, pd.DataFrame]\n) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"Creates a set of tables for defining a `PyPSA` network from a set `ISPyPSA` tables.\n\n    Examples:\n        Perform requried imports.\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; from ispypsa.config import load_config\n        &gt;&gt;&gt; from ispypsa.data_fetch import read_csvs, write_csvs\n        &gt;&gt;&gt; from ispypsa.translator import create_pypsa_friendly_inputs\n\n        Load ISPyPSA model config file and input tables.\n        &gt;&gt;&gt; config = load_config(Path(\"ispypsa_config.yaml\"))\n        &gt;&gt;&gt; ispypsa_input_tables = read_csvs(Path(\"ispypsa_inputs_directory\"))\n\n        Make the PyPSA friendly inputs!\n        &gt;&gt;&gt; pypsa_friendly_inputs = create_pypsa_friendly_inputs(\n        ... config=config,\n        ... ispypsa_tables=ispypsa_input_tables\n        ... )\n\n        Write the resulting dataframes to CSVs.\n        &gt;&gt;&gt; write_csvs(pypsa_friendly_inputs, Path(\"pypsa_friendly_inputs\"))\n\n    Args:\n        config: `ISPyPSA` `ispypsa.config.ModelConfig` object (add link to config docs).\n        ispypsa_tables: dictionary of dataframes providing the `ISPyPSA` input tables.\n            (add link to ispypsa input tables docs).\n\n    Returns: dictionary of dataframes in the `PyPSA` friendly format. (add link to\n        pypsa friendly format table docs)\n    \"\"\"\n    pypsa_inputs = {}\n\n    pypsa_inputs[\"investment_period_weights\"] = _create_investment_period_weightings(\n        config.temporal.capacity_expansion.investment_periods,\n        config.temporal.range.end_year,\n        config.discount_rate,\n    )\n\n    translated_generators = []\n    translated_ecaa_generators = _translate_ecaa_generators(\n        ispypsa_tables,\n        config.temporal.capacity_expansion.investment_periods,\n        config.network.nodes.regional_granularity,\n        config.network.nodes.rezs,\n        config.temporal.year_type,\n    )\n    _append_if_not_empty(translated_generators, translated_ecaa_generators)\n\n    translated_new_entrant_generators = _translate_new_entrant_generators(\n        ispypsa_tables,\n        config.temporal.capacity_expansion.investment_periods,\n        config.discount_rate,\n        config.network.nodes.regional_granularity,\n        config.network.nodes.rezs,\n    )\n    _append_if_not_empty(translated_generators, translated_new_entrant_generators)\n\n    if len(translated_generators) &gt; 0:\n        pypsa_inputs[\"generators\"] = pd.concat(\n            translated_generators,\n            axis=0,\n            ignore_index=True,\n        )\n    else:\n        # TODO: Log, improve error message (/ is this the right place for the error?)\n        raise ValueError(\"No generator data returned from translator.\")\n\n    batteries = []\n    translated_ecaa_batteries = _translate_ecaa_batteries(\n        ispypsa_tables,\n        config.temporal.capacity_expansion.investment_periods,\n        config.network.nodes.regional_granularity,\n        config.network.nodes.rezs,\n        config.temporal.year_type,\n    )\n    _append_if_not_empty(batteries, translated_ecaa_batteries)\n\n    translated_new_entrant_batteries = _translate_new_entrant_batteries(\n        ispypsa_tables,\n        config.temporal.capacity_expansion.investment_periods,\n        config.discount_rate,\n        config.network.nodes.regional_granularity,\n        config.network.nodes.rezs,\n    )\n    _append_if_not_empty(batteries, translated_new_entrant_batteries)\n\n    if len(batteries) &gt; 0:\n        pypsa_inputs[\"batteries\"] = pd.concat(\n            batteries,\n            axis=0,\n            ignore_index=True,\n        )\n    else:\n        logging.warning(\n            \"No battery data returned from translator - no batteries added to model.\"\n        )\n        # raise an error? Improve the message for sure\n\n    buses = []\n    links = []\n\n    if config.network.nodes.regional_granularity == \"sub_regions\":\n        buses.append(_translate_isp_sub_regions_to_buses(ispypsa_tables[\"sub_regions\"]))\n    elif config.network.nodes.regional_granularity == \"nem_regions\":\n        buses.append(_translate_nem_regions_to_buses(ispypsa_tables[\"nem_regions\"]))\n    elif config.network.nodes.regional_granularity == \"single_region\":\n        buses.append(_create_single_region_bus())\n\n    if config.unserved_energy.cost is not None:\n        unserved_energy_generators = _create_unserved_energy_generators(\n            buses[0],  # create generators for just demand buses not rez buses too.\n            config.unserved_energy.cost,\n            config.unserved_energy.max_per_node,\n        )\n        pypsa_inputs[\"generators\"] = pd.concat(\n            [pypsa_inputs[\"generators\"], unserved_energy_generators], ignore_index=True\n        )\n\n    if config.network.nodes.rezs == \"discrete_nodes\":\n        buses.append(_translate_rezs_to_buses(ispypsa_tables[\"renewable_energy_zones\"]))\n        links.append(\n            _translate_renewable_energy_zone_build_limits_to_links(\n                ispypsa_tables[\"renewable_energy_zones\"],\n                ispypsa_tables[\"rez_transmission_expansion_costs\"],\n                config,\n            )\n        )\n\n    if config.network.nodes.regional_granularity != \"single_region\":\n        links.append(_translate_flow_paths_to_links(ispypsa_tables, config))\n\n    pypsa_inputs[\"buses\"] = pd.concat(buses)\n\n    if len(links) &gt; 0:\n        pypsa_inputs[\"links\"] = pd.concat(links)\n    else:\n        pypsa_inputs[\"links\"] = pd.DataFrame()\n\n    pypsa_inputs.update(\n        _translate_custom_constraints(\n            config, ispypsa_tables, pypsa_inputs[\"links\"], pypsa_inputs[\"generators\"]\n        )\n    )\n\n    return pypsa_inputs\n</code></pre>"},{"location":"api/#ispypsa.translator.create_pypsa_friendly_snapshots","title":"ispypsa.translator.create_pypsa_friendly_snapshots","text":"<pre><code>create_pypsa_friendly_snapshots(config: ModelConfig, model_phase: Literal['capacity_expansion', 'operational'], existing_generators: DataFrame | None = None, demand_traces: dict[str, DataFrame] | None = None, generator_traces: dict[str, DataFrame] | None = None) -&gt; pd.DataFrame\n</code></pre> <p>Create a pd.DataFrame of PyPSA-compatible snapshots and associated investment periods for either the 'capacity_expansion' or 'operational' model phase.</p> <p>This function also includes investment_periods for the 'operational' phase (even though they are not strictly required), because PyPSA expects both 'snapshots' and 'investment_periods' when overwriting an existing network.</p> <p>Examples:</p> <p>Perform required imports.</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ispypsa.config import load_config\n&gt;&gt;&gt; from ispypsa.translator import create_pypsa_friendly_snapshots\n</code></pre> <p>Load the ISPyPSA configuration.</p> <pre><code>&gt;&gt;&gt; config = load_config(Path(\"ispypsa_config.yaml\"))\n</code></pre> <p>Create snapshots for capacity expansion modelling.</p> <pre><code>&gt;&gt;&gt; snapshots = create_pypsa_friendly_snapshots(config, \"capacity_expansion\")\n</code></pre> <p>Create snapshots for operational modelling.</p> <pre><code>&gt;&gt;&gt; snapshots = create_pypsa_friendly_snapshots(config, \"operational\")\n</code></pre> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>ModelConfig</code>)           \u2013            <p>ispypsa.ModelConfig instance</p> </li> <li> <code>model_phase</code>               (<code>Literal['capacity_expansion', 'operational']</code>)           \u2013            <p>string defining whether the snapshots are for the operational or capacity expansion phase of the modelling. This allows the correct temporal config inputs to be used from the ModelConfig instance.</p> </li> <li> <code>existing_generators</code>               (<code>DataFrame | None</code>, default:                   <code>None</code> )           \u2013            <p>pd.DataFrame containing existing generators data, required if using named_representative_weeks with residual metrics (e.g., \"residual-peak-demand\", \"residual-minimum-demand\") (optional).</p> </li> <li> <code>demand_traces</code>               (<code>dict[str, DataFrame] | None</code>, default:                   <code>None</code> )           \u2013            <p>dict[str, pd.DataFrame] with demand node names as keys and time series dataframes as values (optional, required for named_representative_weeks).</p> </li> <li> <code>generator_traces</code>               (<code>dict[str, DataFrame] | None</code>, default:                   <code>None</code> )           \u2013            <p>dict[str, pd.DataFrame] with generator names as keys and time series dataframes as values (optional, required for residual metrics).</p> </li> </ul> <p>A pd.DataFrame containing the columns 'investment_periods' (int) defining</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>the investment a modelled inteval belongs to and 'snapshots' (datetime) defining</p> </li> <li> <code>DataFrame</code>           \u2013            <p>each time interval modelled. 'investment_periods' periods are refered to by the</p> </li> <li> <code>DataFrame</code>           \u2013            <p>year (financial or calander) in which they begin.</p> </li> </ul> Source code in <code>src\\ispypsa\\translator\\snapshots.py</code> <pre><code>def create_pypsa_friendly_snapshots(\n    config: ModelConfig,\n    model_phase: Literal[\"capacity_expansion\", \"operational\"],\n    existing_generators: pd.DataFrame | None = None,\n    demand_traces: dict[str, pd.DataFrame] | None = None,\n    generator_traces: dict[str, pd.DataFrame] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Create a pd.DataFrame of PyPSA-compatible snapshots and associated investment\n    periods for either the 'capacity_expansion' or 'operational' model phase.\n\n    This function also includes investment_periods for the 'operational' phase\n    (even though they are not strictly required), because PyPSA expects both 'snapshots'\n    and 'investment_periods' when overwriting an existing network.\n\n    Examples:\n        Perform required imports.\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; from ispypsa.config import load_config\n        &gt;&gt;&gt; from ispypsa.translator import create_pypsa_friendly_snapshots\n\n        Load the ISPyPSA configuration.\n        &gt;&gt;&gt; config = load_config(Path(\"ispypsa_config.yaml\"))\n\n        Create snapshots for capacity expansion modelling.\n        &gt;&gt;&gt; snapshots = create_pypsa_friendly_snapshots(config, \"capacity_expansion\")\n\n        Create snapshots for operational modelling.\n        &gt;&gt;&gt; snapshots = create_pypsa_friendly_snapshots(config, \"operational\")\n\n    Args:\n        config: ispypsa.ModelConfig instance\n        model_phase: string defining whether the snapshots are for the operational or\n            capacity expansion phase of the modelling. This allows the correct temporal\n            config inputs to be used from the ModelConfig instance.\n        existing_generators: pd.DataFrame containing existing generators data, required\n            if using named_representative_weeks with residual metrics (e.g.,\n            \"residual-peak-demand\", \"residual-minimum-demand\") (optional).\n        demand_traces: dict[str, pd.DataFrame] with demand node names as keys and\n            time series dataframes as values (optional, required for named_representative_weeks).\n        generator_traces: dict[str, pd.DataFrame] with generator names as keys and\n            time series dataframes as values (optional, required for residual metrics).\n\n    Returns: A pd.DataFrame containing the columns 'investment_periods' (int) defining\n        the investment a modelled inteval belongs to and 'snapshots' (datetime) defining\n        each time interval modelled. 'investment_periods' periods are refered to by the\n        year (financial or calander) in which they begin.\n    \"\"\"\n    if model_phase == \"capacity_expansion\":\n        resolution_min = config.temporal.capacity_expansion.resolution_min\n        aggregation = config.temporal.capacity_expansion.aggregation\n        investment_periods = config.temporal.capacity_expansion.investment_periods\n    else:\n        resolution_min = config.temporal.operational.resolution_min\n        aggregation = config.temporal.operational.aggregation\n        investment_periods = config.temporal.capacity_expansion.investment_periods\n\n    snapshots = _create_complete_snapshots_index(\n        start_year=config.temporal.range.start_year,\n        end_year=config.temporal.range.end_year,\n        temporal_resolution_min=resolution_min,\n        year_type=config.temporal.year_type,\n    )\n\n    snapshots = _filter_snapshots(\n        config.temporal.year_type,\n        config.temporal.range,\n        aggregation,\n        snapshots,\n        existing_generators=existing_generators,\n        demand_traces=demand_traces,\n        generator_traces=generator_traces,\n    )\n\n    snapshots = _add_investment_periods(\n        snapshots, investment_periods, config.temporal.year_type\n    )\n\n    return snapshots\n</code></pre>"},{"location":"api/#ispypsa.translator.create_pypsa_friendly_timeseries_inputs","title":"ispypsa.translator.create_pypsa_friendly_timeseries_inputs","text":"<pre><code>create_pypsa_friendly_timeseries_inputs(config: ModelConfig, model_phase: Literal['capacity_expansion', 'operational'], ispypsa_tables: dict[str, DataFrame], generators: DataFrame, parsed_traces_directory: Path, pypsa_friendly_timeseries_inputs_location: Path, snapshots: DataFrame | None = None) -&gt; pd.DataFrame\n</code></pre> <p>Creates snapshots and timeseries data files in PyPSA friendly format for generation and demand.</p> <ul> <li> <p>First creates snapshots based on the temporal configuration, optionally using   named_representative_weeks and/or representative_weeks if configured. If snapshots   are provided, they are used instead of generating new ones.</p> </li> <li> <p>a time series file is created for each wind and solar generator in the new_entrant_generators table (table in ispypsa_tables dict). The time series data is saved in parquet files in the 'solar_traces' and 'wind_traces' directories with the columns \"snapshots\" (datetime) and \"p_max_pu\" (float specifying availability in MW).</p> </li> <li> <p>a time series file is created for each generator in the translated generators table (table in pypsa_inputs dict) containing the marginal costs for each generator in each snapshot. The time series data is saved in parquet files in the 'marginal_costs' directory with the columns \"snapshots\" (datetime) and \"marginal_cost\" (float specifying marginal cost in $/MWh).</p> </li> <li> <p>a time series file is created for each model region specifying the load in that region (regions set by config.network.nodes.regional_granularity). The time series data is saved in parquet files in the 'demand_traces' directory with the columns \"snapshots\" (datetime) and \"p_set\" (float specifying load in MW).</p> </li> </ul> <p>Examples:</p> <p>Perform required imports.</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ispypsa.config import load_config\n&gt;&gt;&gt; from ispypsa.data_fetch import read_csvs\n&gt;&gt;&gt; from ispypsa.translator import (\n...     create_pypsa_friendly_inputs,\n...     create_pypsa_friendly_timeseries_inputs\n... )\n</code></pre> <p>Load config and ISPyPSA input tables.</p> <pre><code>&gt;&gt;&gt; config = load_config(Path(\"ispypsa_config.yaml\"))\n&gt;&gt;&gt; ispypsa_tables = read_csvs(Path(\"ispypsa_inputs\"))\n</code></pre> <p>Create PyPSA-friendly inputs to get the generators table.</p> <pre><code>&gt;&gt;&gt; pypsa_friendly_inputs = create_pypsa_friendly_inputs(config, ispypsa_tables)\n</code></pre> <p>Create timeseries inputs for capacity expansion modelling.</p> <pre><code>&gt;&gt;&gt; snapshots = create_pypsa_friendly_timeseries_inputs(\n...     config,\n...     \"capacity_expansion\",\n...     ispypsa_tables,\n...     pypsa_friendly_inputs[\"generators\"],\n...     Path(\"parsed_traces\"),\n...     Path(\"pypsa_friendly/timeseries\")\n... )\n</code></pre> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>ModelConfig</code>)           \u2013            <p>ISPyPSA ModelConfig instance.</p> </li> <li> <code>model_phase</code>               (<code>Literal['capacity_expansion', 'operational']</code>)           \u2013            <p>Either \"capacity_expansion\" or \"operational\". Determines which temporal config settings to use.</p> </li> <li> <code>ispypsa_tables</code>               (<code>dict[str, DataFrame]</code>)           \u2013            <p>Dictionary of ISPyPSA input tables. Must contain ecaa_generators, new_entrant_generators, sub_regions tables, and fuel cost tables for fuel types present in the generator tables.</p> </li> <li> <code>generators</code>               (<code>DataFrame</code>)           \u2013            <p>PyPSA-friendly generators DataFrame (from create_pypsa_friendly_inputs). Used for calculating dynamic marginal costs.</p> </li> <li> <code>parsed_traces_directory</code>               (<code>Path</code>)           \u2013            <p>Path to trace data parsed using isp-trace-parser.</p> </li> <li> <code>pypsa_friendly_timeseries_inputs_location</code>               (<code>Path</code>)           \u2013            <p>Path where timeseries data will be saved.</p> </li> <li> <code>snapshots</code>               (<code>DataFrame | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional pre-defined snapshots DataFrame. If provided, must contain 'snapshots' (datetime) and 'investment_periods' (int) columns.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame containing the snapshots used for the timeseries.</p> </li> </ul> Source code in <code>src\\ispypsa\\translator\\create_pypsa_friendly.py</code> <pre><code>def create_pypsa_friendly_timeseries_inputs(\n    config: ModelConfig,\n    model_phase: Literal[\"capacity_expansion\", \"operational\"],\n    ispypsa_tables: dict[str, pd.DataFrame],\n    generators: pd.DataFrame,\n    parsed_traces_directory: Path,\n    pypsa_friendly_timeseries_inputs_location: Path,\n    snapshots: pd.DataFrame | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Creates snapshots and timeseries data files in PyPSA friendly format for generation\n    and demand.\n\n    - First creates snapshots based on the temporal configuration, optionally using\n      named_representative_weeks and/or representative_weeks if configured. If snapshots\n      are provided, they are used instead of generating new ones.\n\n    - a time series file is created for each wind and solar generator in the new_entrant_generators\n    table (table in ispypsa_tables dict). The time series data is saved in parquet files\n    in the 'solar_traces' and 'wind_traces' directories with the columns \"snapshots\"\n    (datetime) and \"p_max_pu\" (float specifying availability in MW).\n\n    - a time series file is created for each generator in the translated generators table\n    (table in pypsa_inputs dict) containing the marginal costs for each generator in each\n    snapshot. The time series data is saved in parquet files in the 'marginal_costs' directory\n    with the columns \"snapshots\" (datetime) and \"marginal_cost\" (float specifying marginal\n    cost in $/MWh).\n\n    - a time series file is created for each model region specifying the load in that\n    region (regions set by config.network.nodes.regional_granularity). The time series\n    data is saved in parquet files in the 'demand_traces' directory with the columns\n    \"snapshots\" (datetime) and \"p_set\" (float specifying load in MW).\n\n    Examples:\n        Perform required imports.\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; from ispypsa.config import load_config\n        &gt;&gt;&gt; from ispypsa.data_fetch import read_csvs\n        &gt;&gt;&gt; from ispypsa.translator import (\n        ...     create_pypsa_friendly_inputs,\n        ...     create_pypsa_friendly_timeseries_inputs\n        ... )\n\n        Load config and ISPyPSA input tables.\n        &gt;&gt;&gt; config = load_config(Path(\"ispypsa_config.yaml\"))\n        &gt;&gt;&gt; ispypsa_tables = read_csvs(Path(\"ispypsa_inputs\"))\n\n        Create PyPSA-friendly inputs to get the generators table.\n        &gt;&gt;&gt; pypsa_friendly_inputs = create_pypsa_friendly_inputs(config, ispypsa_tables)\n\n        Create timeseries inputs for capacity expansion modelling.\n        &gt;&gt;&gt; snapshots = create_pypsa_friendly_timeseries_inputs(\n        ...     config,\n        ...     \"capacity_expansion\",\n        ...     ispypsa_tables,\n        ...     pypsa_friendly_inputs[\"generators\"],\n        ...     Path(\"parsed_traces\"),\n        ...     Path(\"pypsa_friendly/timeseries\")\n        ... )\n\n    Args:\n        config: ISPyPSA ModelConfig instance.\n        model_phase: Either \"capacity_expansion\" or \"operational\". Determines which\n            temporal config settings to use.\n        ispypsa_tables: Dictionary of ISPyPSA input tables. Must contain\n            ecaa_generators, new_entrant_generators, sub_regions tables, and fuel\n            cost tables for fuel types present in the generator tables.\n        generators: PyPSA-friendly generators DataFrame (from create_pypsa_friendly_inputs).\n            Used for calculating dynamic marginal costs.\n        parsed_traces_directory: Path to trace data parsed using isp-trace-parser.\n        pypsa_friendly_timeseries_inputs_location: Path where timeseries data will be saved.\n        snapshots: Optional pre-defined snapshots DataFrame. If provided, must contain\n            'snapshots' (datetime) and 'investment_periods' (int) columns.\n\n    Returns:\n        pd.DataFrame containing the snapshots used for the timeseries.\n    \"\"\"\n\n    if model_phase == \"capacity_expansion\":\n        reference_year_cycle = config.temporal.capacity_expansion.reference_year_cycle\n    else:\n        reference_year_cycle = config.temporal.operational.reference_year_cycle\n\n    reference_year_mapping = construct_reference_year_mapping(\n        start_year=config.temporal.range.start_year,\n        end_year=config.temporal.range.end_year,\n        reference_years=reference_year_cycle,\n    )\n\n    # Load generator timeseries data (organized by type)\n    generator_traces_by_type = create_pypsa_friendly_ecaa_generator_timeseries(\n        ispypsa_tables[\"ecaa_generators\"],\n        parsed_traces_directory,\n        generator_types=[\"solar\", \"wind\"],\n        reference_year_mapping=reference_year_mapping,\n        year_type=config.temporal.year_type,\n    )\n\n    # Load demand timeseries data\n    demand_traces = create_pypsa_friendly_bus_demand_timeseries(\n        ispypsa_tables[\"sub_regions\"],\n        parsed_traces_directory,\n        scenario=config.scenario,\n        regional_granularity=config.network.nodes.regional_granularity,\n        reference_year_mapping=reference_year_mapping,\n        year_type=config.temporal.year_type,\n    )\n\n    # Use provided snapshots or create new ones\n    if snapshots is None:\n        # Create snapshots, potentially using the loaded data for named_representative_weeks\n        # Flatten generator traces for snapshot creation\n        all_generator_traces = _flatten_generator_traces(generator_traces_by_type)\n\n        snapshots = create_pypsa_friendly_snapshots(\n            config,\n            model_phase,\n            existing_generators=ispypsa_tables.get(\"ecaa_generators\"),\n            demand_traces=demand_traces,\n            generator_traces=all_generator_traces,\n        )\n\n    if generator_traces_by_type is not None:\n        # Filter and save generator timeseries by type\n        for gen_type, gen_traces in generator_traces_by_type.items():\n            if gen_traces:\n                _filter_and_save_timeseries(\n                    gen_traces,\n                    snapshots,\n                    pypsa_friendly_timeseries_inputs_location,\n                    f\"{gen_type}_traces\",\n                )\n\n    # Filter and save demand timeseries\n    _filter_and_save_timeseries(\n        demand_traces,\n        snapshots,\n        pypsa_friendly_timeseries_inputs_location,\n        \"demand_traces\",\n    )\n\n    create_pypsa_friendly_new_entrant_generator_timeseries(\n        ispypsa_tables[\"new_entrant_generators\"],\n        parsed_traces_directory,\n        pypsa_friendly_timeseries_inputs_location,\n        generator_types=[\"solar\", \"wind\"],\n        reference_year_mapping=reference_year_mapping,\n        year_type=config.temporal.year_type,\n        snapshots=snapshots,\n    )\n\n    # This is needed because numbers can be converted to strings if the data has been saved to a csv.\n    generators = convert_to_numeric_if_possible(generators, cols=[\"marginal_cost\"])\n    # NOTE - maybe this function needs to be somewhere separate/handled a little\n    # different because it currently requires the translated generator table as input?\n    create_pypsa_friendly_dynamic_marginal_costs(\n        ispypsa_tables,\n        generators,\n        snapshots,\n        pypsa_friendly_timeseries_inputs_location,\n    )\n\n    snapshots = _add_snapshot_weightings(\n        snapshots, config.temporal.capacity_expansion.resolution_min\n    )\n\n    return snapshots\n</code></pre>"},{"location":"api/#model-building-execution","title":"Model Building &amp; Execution","text":""},{"location":"api/#ispypsa.pypsa_build.build_pypsa_network","title":"ispypsa.pypsa_build.build_pypsa_network","text":"<pre><code>build_pypsa_network(pypsa_friendly_tables: dict[str:(DataFrame)], path_to_pypsa_friendly_timeseries_data: Path)\n</code></pre> <p>Creates a <code>pypsa.Network</code> based on set of pypsa friendly input tables.</p> <p>Examples:</p> <p>Peform required imports.</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ispypsa.data_fetch import read_csvs, write_csvs\n&gt;&gt;&gt; from ispypsa.pypsa_build import build_pypsa_network\n</code></pre> <p>Read in PyPSA friendly tables from CSV.</p> <pre><code>&gt;&gt;&gt; pypsa_input_tables = read_csvs(Path(\"pypsa_friendly_inputs_directory\"))\n</code></pre> <pre><code>&gt;&gt;&gt; pypsa_friendly_inputs = build_pypsa_network(\n... pypsa_friendly_tables=pypsa_input_tables,\n... path_to_pypsa_friendly_timeseries_data=Path(\"pypsa_friendly_timeseries_data\")\n... )\n</code></pre> <p>Then the model can be run in PyPSA</p> <pre><code>&gt;&gt;&gt; network.optimize.solve_model(solver_name=\"highs\")\n</code></pre> <p>And the results saved to disk.</p> <pre><code>&gt;&gt;&gt; network.export_to_netcdf(Path(\"model_results.nc\"))\n</code></pre> <p>Parameters:</p> <ul> <li> <code>pypsa_friendly_tables</code>               (<code>dict[str:(DataFrame)]</code>)           \u2013            <p>dictionary of dataframes in the <code>PyPSA</code> friendly format. (add link to pypsa friendly format table docs)</p> </li> <li> <code>path_to_pypsa_friendly_timeseries_data</code>               (<code>Path</code>)           \u2013            <p><code>Path</code> to <code>PyPSA</code> friendly time series data (add link to timeseries data docs.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>pypsa.Network: A PyPSA network object ready for optimisation.</p> </li> </ul> Source code in <code>src\\ispypsa\\pypsa_build\\build.py</code> <pre><code>def build_pypsa_network(\n    pypsa_friendly_tables: dict[str : pd.DataFrame],\n    path_to_pypsa_friendly_timeseries_data: Path,\n):\n    \"\"\"Creates a `pypsa.Network` based on set of pypsa friendly input tables.\n\n    Examples:\n        Peform required imports.\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; from ispypsa.data_fetch import read_csvs, write_csvs\n        &gt;&gt;&gt; from ispypsa.pypsa_build import build_pypsa_network\n\n        Read in PyPSA friendly tables from CSV.\n        &gt;&gt;&gt; pypsa_input_tables = read_csvs(Path(\"pypsa_friendly_inputs_directory\"))\n\n        &gt;&gt;&gt; pypsa_friendly_inputs = build_pypsa_network(\n        ... pypsa_friendly_tables=pypsa_input_tables,\n        ... path_to_pypsa_friendly_timeseries_data=Path(\"pypsa_friendly_timeseries_data\")\n        ... )\n\n        Then the model can be run in PyPSA\n        &gt;&gt;&gt; network.optimize.solve_model(solver_name=\"highs\")\n\n        And the results saved to disk.\n        &gt;&gt;&gt; network.export_to_netcdf(Path(\"model_results.nc\"))\n\n    Args:\n        pypsa_friendly_tables: dictionary of dataframes in the `PyPSA` friendly format.\n            (add link to pypsa friendly format table docs)\n        path_to_pypsa_friendly_timeseries_data: `Path` to `PyPSA` friendly time series\n            data (add link to timeseries data docs.\n\n    Returns:\n        pypsa.Network: A PyPSA network object ready for optimisation.\n    \"\"\"\n    network = _initialise_network(pypsa_friendly_tables[\"snapshots\"])\n\n    _add_investment_period_weights(\n        network, pypsa_friendly_tables[\"investment_period_weights\"]\n    )\n\n    _add_carriers_to_network(\n        network,\n        pypsa_friendly_tables.get(\"generators\"),\n        pypsa_friendly_tables.get(\"batteries\"),\n    )\n\n    _add_buses_to_network(\n        network, pypsa_friendly_tables[\"buses\"], path_to_pypsa_friendly_timeseries_data\n    )\n\n    if \"links\" in pypsa_friendly_tables.keys():\n        _add_links_to_network(network, pypsa_friendly_tables[\"links\"])\n\n    _add_generators_to_network(\n        network,\n        pypsa_friendly_tables[\"generators\"],\n        path_to_pypsa_friendly_timeseries_data,\n    )\n\n    if \"batteries\" in pypsa_friendly_tables.keys():\n        _add_batteries_to_network(network, pypsa_friendly_tables[\"batteries\"])\n\n    if \"custom_constraints_generators\" in pypsa_friendly_tables.keys():\n        _add_bus_for_custom_constraints(network)\n\n        _add_custom_constraint_generators_to_network(\n            network, pypsa_friendly_tables[\"custom_constraints_generators\"]\n        )\n\n    # The underlying linopy model needs to get built so we can add custom constraints.\n    network.optimize.create_model(multi_investment_periods=True)\n\n    if \"custom_constraints_rhs\" in pypsa_friendly_tables:\n        _add_custom_constraints(\n            network,\n            pypsa_friendly_tables[\"custom_constraints_rhs\"],\n            pypsa_friendly_tables[\"custom_constraints_lhs\"],\n        )\n\n    return network\n</code></pre>"},{"location":"api/#ispypsa.pypsa_build.update_network_timeseries","title":"ispypsa.pypsa_build.update_network_timeseries","text":"<pre><code>update_network_timeseries(network: Network, pypsa_friendly_input_tables: dict[str, DataFrame], snapshots: DataFrame, pypsa_friendly_timeseries_location: Path) -&gt; None\n</code></pre> <p>Update the time series data in a pypsa.Network instance.</p> <p>Designed to help convert capacity expansion network models into operational models but may also be useful in other circumstances, such as when running a capacity expansion model with different reference year cycles.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ispypsa.data_fetch import read_csvs\n&gt;&gt;&gt; from ispypsa.pypsa_build import update_network_timeseries\n</code></pre> <p>Get PyPSA friendly inputs (inparticular these need to contain the generators and buses tables).</p> <pre><code>&gt;&gt;&gt; pypsa_friendly_input_tables = read_csvs(\"path/to/pypsa/friendly/inputs\")\n</code></pre> <p>Get the snapshots for the updated time series data.</p> <pre><code>&gt;&gt;&gt; snapshots = pd.read_csv(\"new_snapshots.csv\")\n</code></pre> <p>Get the pypsa.Network we want to update the time series data in.</p> <pre><code>&gt;&gt;&gt; network = pypsa.Network()\n&gt;&gt;&gt; network.import_from_netcdf(\"existing_network.nc\")\n</code></pre> <p>Create pd.Dataframe defining the set of snapshot (time intervals) to be used.</p> <pre><code>&gt;&gt;&gt; update_network_timeseries(\n...     network,\n...     pypsa_friendly_input_tables,\n...     snapshots,\n...     Path(\"path/to/time/series/data/files\")\n... )\n</code></pre> <p>Parameters:</p> <ul> <li> <code>network</code>               (<code>Network</code>)           \u2013            <p>pypsa.Network which has set of generators, loads, and buses consistent with the updated time series data. i.e. if generator 'Y' exists in the existing network it also needs to exist in the updated time series data.</p> </li> <li> <code>pypsa_friendly_input_tables</code>               (<code>dict[str, DataFrame]</code>)           \u2013            <p>dictionary of dataframes in the <code>PyPSA</code> friendly format. (add link to pypsa friendly format table docs)</p> </li> <li> <code>snapshots</code>               (<code>DataFrame</code>)           \u2013            <p>a pd.DataFrame containing the columns 'investment_periods' (int) defining the investment a modelled inteval belongs to and 'snapshots' (datetime) defining each time interval modelled. 'investment_periods' periods are refered to by the year (financial or calander) in which they begin.</p> </li> <li> <code>pypsa_friendly_timeseries_location</code>               (<code>Path</code>)           \u2013            <p><code>Path</code> to <code>PyPSA</code> friendly time series data (add link to timeseries data docs).</p> </li> </ul> <p>Returns: None</p> Source code in <code>src\\ispypsa\\pypsa_build\\update.py</code> <pre><code>def update_network_timeseries(\n    network: pypsa.Network,\n    pypsa_friendly_input_tables: dict[str, pd.DataFrame],\n    snapshots: pd.DataFrame,\n    pypsa_friendly_timeseries_location: Path,\n) -&gt; None:\n    \"\"\"\n    Update the time series data in a pypsa.Network instance.\n\n    Designed to help convert capacity expansion network models into operational models\n    but may also be useful in other circumstances, such as when running a capacity\n    expansion model with different reference year cycles.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; from ispypsa.data_fetch import read_csvs\n        &gt;&gt;&gt; from ispypsa.pypsa_build import update_network_timeseries\n\n        Get PyPSA friendly inputs (inparticular these need to contain the generators and\n        buses tables).\n\n        &gt;&gt;&gt; pypsa_friendly_input_tables = read_csvs(\"path/to/pypsa/friendly/inputs\")\n\n        Get the snapshots for the updated time series data.\n\n        &gt;&gt;&gt; snapshots = pd.read_csv(\"new_snapshots.csv\")\n\n        Get the pypsa.Network we want to update the time series data in.\n\n        &gt;&gt;&gt; network = pypsa.Network()\n        &gt;&gt;&gt; network.import_from_netcdf(\"existing_network.nc\")\n\n        Create pd.Dataframe defining the set of snapshot (time intervals) to be used.\n\n        &gt;&gt;&gt; update_network_timeseries(\n        ...     network,\n        ...     pypsa_friendly_input_tables,\n        ...     snapshots,\n        ...     Path(\"path/to/time/series/data/files\")\n        ... )\n\n    Args:\n        network: pypsa.Network which has set of generators, loads, and buses consistent\n            with the updated time series data. i.e. if generator 'Y' exists in the\n            existing network it also needs to exist in the updated time series data.\n        pypsa_friendly_input_tables: dictionary of dataframes in the `PyPSA` friendly\n            format. (add link to pypsa friendly format table docs)\n        snapshots: a pd.DataFrame containing the columns 'investment_periods' (int)\n            defining the investment a modelled inteval belongs to and 'snapshots'\n            (datetime) defining each time interval modelled. 'investment_periods'\n            periods are refered to by the year (financial or calander) in which they\n            begin.\n        pypsa_friendly_timeseries_location: `Path` to `PyPSA` friendly time series\n            data (add link to timeseries data docs).\n\n    Returns: None\n    \"\"\"\n    snapshots[\"snapshots\"] = pd.to_datetime(snapshots[\"snapshots\"])\n    snapshots_as_indexes = pd.MultiIndex.from_arrays(\n        [snapshots[\"investment_periods\"], snapshots[\"snapshots\"]]\n    )\n    network.snapshots = snapshots_as_indexes\n    network.set_investment_periods(snapshots[\"investment_periods\"].unique())\n    _update_generators_availability_timeseries(\n        network,\n        pypsa_friendly_input_tables[\"generators\"],\n        pypsa_friendly_timeseries_location,\n    )\n    _update_buses_demand_timeseries(\n        network,\n        pypsa_friendly_input_tables[\"buses\"],\n        pypsa_friendly_timeseries_location,\n    )\n\n    # The underlying linopy model needs to get built again here so that the new time\n    # series data is used in the linopy model rather than the old data.\n    network.optimize.create_model()\n\n    # As we rebuilt the linopy model now we need to re add custom constrains.\n    _add_custom_constraints(\n        network,\n        pypsa_friendly_input_tables[\"custom_constraints_rhs\"],\n        pypsa_friendly_input_tables[\"custom_constraints_lhs\"],\n    )\n</code></pre>"},{"location":"api/#ispypsa.pypsa_build.save_pypsa_network","title":"ispypsa.pypsa_build.save_pypsa_network","text":"<pre><code>save_pypsa_network(network: Network, save_directory: Path, save_name: str) -&gt; None\n</code></pre> <p>Save the optimised PyPSA network as a NetCDF file.</p> <p>Examples:</p> <p>Perform required imports.</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ispypsa.pypsa_build import save_pypsa_network\n</code></pre> <p>After running the model optimisation, save the network.</p> <pre><code>&gt;&gt;&gt; network.optimize.solve_model(solver_name=\"highs\")\n&gt;&gt;&gt; save_pypsa_network(\n...     network,\n...     save_directory=Path(\"outputs\"),\n...     save_name=\"capacity_expansion\"\n... )\n# Saves to outputs/capacity_expansion.nc\n</code></pre> <p>Save operational model results.</p> <pre><code>&gt;&gt;&gt; save_pypsa_network(\n...     network,\n...     save_directory=Path(\"outputs\"),\n...     save_name=\"operational\"\n... )\n# Saves to outputs/operational.nc\n</code></pre> <p>Parameters:</p> <ul> <li> <code>network</code>               (<code>Network</code>)           \u2013            <p>The solved PyPSA network object.</p> </li> <li> <code>save_directory</code>               (<code>Path</code>)           \u2013            <p>Directory where the network file should be saved.</p> </li> <li> <code>save_name</code>               (<code>str</code>)           \u2013            <p>Name for the saved file (without .nc extension).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None</p> </li> </ul> Source code in <code>src\\ispypsa\\pypsa_build\\save.py</code> <pre><code>def save_pypsa_network(\n    network: pypsa.Network, save_directory: Path, save_name: str\n) -&gt; None:\n    \"\"\"Save the optimised PyPSA network as a NetCDF file.\n\n    Examples:\n        Perform required imports.\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; from ispypsa.pypsa_build import save_pypsa_network\n\n        After running the model optimisation, save the network.\n        &gt;&gt;&gt; network.optimize.solve_model(solver_name=\"highs\")\n        &gt;&gt;&gt; save_pypsa_network(\n        ...     network,\n        ...     save_directory=Path(\"outputs\"),\n        ...     save_name=\"capacity_expansion\"\n        ... )\n        # Saves to outputs/capacity_expansion.nc\n\n        Save operational model results.\n        &gt;&gt;&gt; save_pypsa_network(\n        ...     network,\n        ...     save_directory=Path(\"outputs\"),\n        ...     save_name=\"operational\"\n        ... )\n        # Saves to outputs/operational.nc\n\n    Args:\n        network: The solved PyPSA network object.\n        save_directory: Directory where the network file should be saved.\n        save_name: Name for the saved file (without .nc extension).\n\n    Returns:\n        None\n    \"\"\"\n    network.export_to_netcdf(Path(save_directory, f\"{save_name}.nc\"))\n</code></pre>"},{"location":"api/#tabular-results-extraction","title":"Tabular Results Extraction","text":""},{"location":"api/#ispypsa.results.extract_tabular_results","title":"ispypsa.results.extract_tabular_results","text":"<pre><code>extract_tabular_results(network: Network, ispypsa_tables: dict[str, DataFrame]) -&gt; dict[str:(pd.DataFrame)]\n</code></pre> <p>Extract the results from the PyPSA network and return a dictionary of results.</p> <p>Extracts generation expansion, transmission expansion, dispatch, demand, and transmission flow results from the solved PyPSA network.</p> <p>Examples:</p> <p>Perform required imports.</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ispypsa.data_fetch import read_csvs, write_csvs\n&gt;&gt;&gt; from ispypsa.results import extract_tabular_results\n</code></pre> <p>Load ISPyPSA input tables (needed for regions mapping).</p> <pre><code>&gt;&gt;&gt; ispypsa_tables = read_csvs(Path(\"ispypsa_inputs\"))\n</code></pre> <p>After solving the network, extract the results.</p> <pre><code>&gt;&gt;&gt; network.optimize.solve_model(solver_name=\"highs\")\n&gt;&gt;&gt; results = extract_tabular_results(network, ispypsa_tables)\n</code></pre> <p>Access specific result tables.</p> <pre><code>&gt;&gt;&gt; generation_expansion = results[\"generation_expansion\"]\n&gt;&gt;&gt; transmission_flows = results[\"transmission_flows\"]\n</code></pre> <p>Write results to CSV files.</p> <pre><code>&gt;&gt;&gt; write_csvs(results, Path(\"outputs/results\"))\n</code></pre> <p>Parameters:</p> <ul> <li> <code>network</code>               (<code>Network</code>)           \u2013            <p>The PyPSA network object.</p> </li> <li> <code>ispypsa_tables</code>               (<code>dict[str, DataFrame]</code>)           \u2013            <p>Dictionary of ISPyPSA input tables (needed for regions mapping).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str:(DataFrame)]</code>           \u2013            <p>A dictionary of results with the file name as the key and the results as the value.</p> </li> </ul> Source code in <code>src\\ispypsa\\results\\extract.py</code> <pre><code>def extract_tabular_results(\n    network: pypsa.Network,\n    ispypsa_tables: dict[str, pd.DataFrame],\n) -&gt; dict[str : pd.DataFrame]:\n    \"\"\"Extract the results from the PyPSA network and return a dictionary of results.\n\n    Extracts generation expansion, transmission expansion, dispatch, demand, and\n    transmission flow results from the solved PyPSA network.\n\n    Examples:\n        Perform required imports.\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; from ispypsa.data_fetch import read_csvs, write_csvs\n        &gt;&gt;&gt; from ispypsa.results import extract_tabular_results\n\n        Load ISPyPSA input tables (needed for regions mapping).\n        &gt;&gt;&gt; ispypsa_tables = read_csvs(Path(\"ispypsa_inputs\"))\n\n        After solving the network, extract the results.\n        &gt;&gt;&gt; network.optimize.solve_model(solver_name=\"highs\")\n        &gt;&gt;&gt; results = extract_tabular_results(network, ispypsa_tables)\n\n        Access specific result tables.\n        &gt;&gt;&gt; generation_expansion = results[\"generation_expansion\"]\n        &gt;&gt;&gt; transmission_flows = results[\"transmission_flows\"]\n\n        Write results to CSV files.\n        &gt;&gt;&gt; write_csvs(results, Path(\"outputs/results\"))\n\n    Args:\n        network: The PyPSA network object.\n        ispypsa_tables: Dictionary of ISPyPSA input tables (needed for regions mapping).\n\n    Returns:\n        A dictionary of results with the file name as the key and the results as the value.\n    \"\"\"\n\n    # Functions that require link_flows and regions_mapping parameters\n    geographic_transmission_functions = {\n        \"rez_transmission_flows\",\n        \"isp_sub_region_transmission_flows\",\n        \"nem_region_transmission_flows\",\n    }\n\n    results = {}\n\n    # Extract regions and zones mapping to be used in other functions that require it.\n    results[\"regions_and_zones_mapping\"] = extract_regions_and_zones_mapping(\n        ispypsa_tables\n    )\n\n    # Extract first transmission flows to be used in other functions that require it.\n    results[\"transmission_flows\"] = extract_transmission_flows(network)\n\n    for file, function in RESULTS_FILES.items():\n        if file in [\"transmission_flows\", \"regions_and_zones_mapping\"]:\n            continue\n\n        if file in geographic_transmission_functions:\n            results[file] = function(\n                results[\"transmission_flows\"], results[\"regions_and_zones_mapping\"]\n            )\n        else:\n            results[file] = function(network)\n\n    return results\n</code></pre>"},{"location":"api/#plotting","title":"Plotting","text":""},{"location":"api/#ispypsa.plotting.create_plot_suite","title":"ispypsa.plotting.create_plot_suite","text":"<pre><code>create_plot_suite(results: dict[str, DataFrame]) -&gt; dict[Path, dict]\n</code></pre> <p>Create a suite of plots for ISPyPSA modelling results.</p> <p>Works for both capacity expansion and operational model results.</p> <p>Examples:</p> <p>Perform required imports.</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ispypsa.data_fetch import read_csvs\n&gt;&gt;&gt; from ispypsa.results import extract_tabular_results\n&gt;&gt;&gt; from ispypsa.plotting import create_plot_suite, save_plots\n</code></pre> <p>Extract tabular results from the solved network.</p> <pre><code>&gt;&gt;&gt; ispypsa_tables = read_csvs(Path(\"ispypsa_inputs\"))\n&gt;&gt;&gt; results = extract_tabular_results(network, ispypsa_tables)\n</code></pre> <p>Create the plot suite from the results.</p> <pre><code>&gt;&gt;&gt; plots = create_plot_suite(results)\n</code></pre> <p>Save the plots to disk.</p> <pre><code>&gt;&gt;&gt; save_plots(plots, Path(\"outputs/plots\"))\n</code></pre> <p>Parameters:</p> <ul> <li> <code>results</code>               (<code>dict[str, DataFrame]</code>)           \u2013            <p>A dictionary of tabular results from the ISPyPSA model. Should contain: - transmission_expansion - transmission_flows - nem_region_transmission_flows - isp_sub_region_transmission_flows - regions_and_zones_mapping - generator_dispatch - generation_expansion - demand</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[Path, dict]</code>           \u2013            <p>A dictionary of plots with the file path as the key and the plot as the value.</p> </li> </ul> Source code in <code>src\\ispypsa\\plotting\\plot.py</code> <pre><code>def create_plot_suite(\n    results: dict[str, pd.DataFrame],\n) -&gt; dict[Path, dict]:\n    \"\"\"Create a suite of plots for ISPyPSA modelling results.\n\n    Works for both capacity expansion and operational model results.\n\n    Examples:\n        Perform required imports.\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; from ispypsa.data_fetch import read_csvs\n        &gt;&gt;&gt; from ispypsa.results import extract_tabular_results\n        &gt;&gt;&gt; from ispypsa.plotting import create_plot_suite, save_plots\n\n        Extract tabular results from the solved network.\n        &gt;&gt;&gt; ispypsa_tables = read_csvs(Path(\"ispypsa_inputs\"))\n        &gt;&gt;&gt; results = extract_tabular_results(network, ispypsa_tables)\n\n        Create the plot suite from the results.\n        &gt;&gt;&gt; plots = create_plot_suite(results)\n\n        Save the plots to disk.\n        &gt;&gt;&gt; save_plots(plots, Path(\"outputs/plots\"))\n\n    Args:\n        results: A dictionary of tabular results from the ISPyPSA model. Should contain:\n            - transmission_expansion\n            - transmission_flows\n            - nem_region_transmission_flows\n            - isp_sub_region_transmission_flows\n            - regions_and_zones_mapping\n            - generator_dispatch\n            - generation_expansion\n            - demand\n\n    Returns:\n        A dictionary of plots with the file path as the key and the plot as the value.\n    \"\"\"\n    nem_region_flows = results.get(\"nem_region_transmission_flows\", pd.DataFrame())\n    isp_sub_region_flows = results.get(\n        \"isp_sub_region_transmission_flows\", pd.DataFrame()\n    )\n\n    plots = {\n        \"transmission\": {\n            \"aggregate_capacity\": plot_aggregate_transmission_capacity(\n                results[\"transmission_expansion\"], results[\"regions_and_zones_mapping\"]\n            ),\n            \"flows\": plot_flows(\n                results[\"transmission_flows\"], results[\"transmission_expansion\"]\n            ),\n            \"regional_expansion\": plot_regional_capacity_expansion(\n                results[\"transmission_expansion\"], results[\"regions_and_zones_mapping\"]\n            ),\n        },\n        \"generation\": plot_generation_capacity_expansion(\n            results[\"generation_expansion\"], results[\"regions_and_zones_mapping\"]\n        ),\n        \"dispatch\": {\n            \"system\": plot_dispatch(\n                results[\"generator_dispatch\"],\n                results[\"demand\"],\n            ),\n            \"regional\": plot_dispatch(\n                results[\"generator_dispatch\"],\n                results[\"demand\"],\n                results[\"regions_and_zones_mapping\"],\n                \"nem_region_id\",\n                nem_region_flows,\n            ),\n            \"sub_regional\": plot_dispatch(\n                results[\"generator_dispatch\"],\n                results[\"demand\"],\n                results[\"regions_and_zones_mapping\"],\n                \"isp_sub_region_id\",\n                isp_sub_region_flows,\n            ),\n        },\n    }\n    return flatten_dict_with_file_paths_as_keys(plots)\n</code></pre>"},{"location":"api/#ispypsa.plotting.save_plots","title":"ispypsa.plotting.save_plots","text":"<pre><code>save_plots(charts: dict[Path, dict], base_path: Path) -&gt; None\n</code></pre> <p>Save a suite of Plotly plots and their underlying data to the plots directory.</p> <p>All plots are saved as interactive HTML files with accompanying CSV data files.</p> <p>Examples:</p> <p>Perform required imports.</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ispypsa.plotting import create_plot_suite, save_plots\n</code></pre> <p>Create plots from results (see <code>create_plot_suite</code> for how to get results).</p> <pre><code>&gt;&gt;&gt; plots = create_plot_suite(results)\n</code></pre> <p>Save all plots to a directory.</p> <pre><code>&gt;&gt;&gt; save_plots(plots, Path(\"outputs/capacity_expansion_plots\"))\n# Creates HTML files like:\n# outputs/capacity_expansion_plots/transmission/aggregate_capacity.html\n# outputs/capacity_expansion_plots/transmission/aggregate_capacity.csv\n# outputs/capacity_expansion_plots/generation.html\n# outputs/capacity_expansion_plots/generation.csv\n</code></pre> <p>Parameters:</p> <ul> <li> <code>charts</code>               (<code>dict[Path, dict]</code>)           \u2013            <p>A dictionary with file paths as keys and dicts with \"plot\" and \"data\" as values.</p> </li> <li> <code>base_path</code>               (<code>Path</code>)           \u2013            <p>The path to the directory where the plots are saved.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None</p> </li> </ul> Source code in <code>src\\ispypsa\\plotting\\plot.py</code> <pre><code>def save_plots(charts: dict[Path, dict], base_path: Path) -&gt; None:\n    \"\"\"Save a suite of Plotly plots and their underlying data to the plots directory.\n\n    All plots are saved as interactive HTML files with accompanying CSV data files.\n\n    Examples:\n        Perform required imports.\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; from ispypsa.plotting import create_plot_suite, save_plots\n\n        Create plots from results (see `create_plot_suite` for how to get results).\n        &gt;&gt;&gt; plots = create_plot_suite(results)\n\n        Save all plots to a directory.\n        &gt;&gt;&gt; save_plots(plots, Path(\"outputs/capacity_expansion_plots\"))\n        # Creates HTML files like:\n        # outputs/capacity_expansion_plots/transmission/aggregate_capacity.html\n        # outputs/capacity_expansion_plots/transmission/aggregate_capacity.csv\n        # outputs/capacity_expansion_plots/generation.html\n        # outputs/capacity_expansion_plots/generation.csv\n\n    Args:\n        charts: A dictionary with file paths as keys and dicts with \"plot\" and \"data\" as values.\n        base_path: The path to the directory where the plots are saved.\n\n    Returns:\n        None\n    \"\"\"\n    for path, content in charts.items():\n        plot = content[\"plot\"]\n\n        # Save Plotly chart as HTML\n        html_path = base_path / path\n        html_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Save the underlying data (CSV)\n        csv_path = html_path.with_suffix(\".csv\")\n        content[\"data\"].to_csv(csv_path, index=False)\n\n        # Save the plot (HTML) with responsive sizing\n        plot.write_html(\n            html_path,\n            full_html=True,\n            include_plotlyjs=True,\n            config={\"responsive\": True},\n        )\n</code></pre>"},{"location":"api/#ispypsa.plotting.generate_results_website","title":"ispypsa.plotting.generate_results_website","text":"<pre><code>generate_results_website(plots: Dict[Path, dict], plots_dir: Path, output_dir: Path, site_name: str = 'ISPyPSA Results', output_filename: str = 'results_viewer.html', subtitle: str = 'Capacity Expansion Analysis', regions_and_zones_mapping: Optional[DataFrame] = None) -&gt; None\n</code></pre> <p>Generate a static website for navigating ISPyPSA plot results.</p> <p>Creates a single HTML file with a navigation pane that mirrors the directory structure of the plots.</p> <p>Examples:</p> <p>Perform required imports.</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ispypsa.data_fetch import read_csvs\n&gt;&gt;&gt; from ispypsa.results import extract_tabular_results\n&gt;&gt;&gt; from ispypsa.plotting import (\n...     create_plot_suite,\n...     save_plots,\n...     generate_results_website\n... )\n</code></pre> <p>Extract results and create plots.</p> <pre><code>&gt;&gt;&gt; ispypsa_tables = read_csvs(Path(\"ispypsa_inputs\"))\n&gt;&gt;&gt; results = extract_tabular_results(network, ispypsa_tables)\n&gt;&gt;&gt; plots = create_plot_suite(results)\n</code></pre> <p>Save plots to disk first.</p> <pre><code>&gt;&gt;&gt; plots_dir = Path(\"outputs/capacity_expansion_plots\")\n&gt;&gt;&gt; save_plots(plots, plots_dir)\n</code></pre> <p>Generate the results website.</p> <pre><code>&gt;&gt;&gt; generate_results_website(\n...     plots,\n...     plots_dir,\n...     output_dir=Path(\"outputs\"),\n...     output_filename=\"capacity_expansion_results_viewer.html\",\n...     subtitle=\"Capacity Expansion Analysis\",\n...     regions_and_zones_mapping=results[\"regions_and_zones_mapping\"]\n... )\n# Creates outputs/capacity_expansion_results_viewer.html\n</code></pre> <p>Parameters:</p> <ul> <li> <code>plots</code>               (<code>Dict[Path, dict]</code>)           \u2013            <p>Dictionary with Path keys (plot file paths) and dict values    containing \"plot\" and \"data\" keys (output from create_*_plot_suite)</p> </li> <li> <code>plots_dir</code>               (<code>Path</code>)           \u2013            <p>Directory where plots are saved (e.g., outputs/capacity_expansion_plots)</p> </li> <li> <code>output_dir</code>               (<code>Path</code>)           \u2013            <p>Directory where the website HTML should be saved (e.g., outputs directory)</p> </li> <li> <code>site_name</code>               (<code>str</code>, default:                   <code>'ISPyPSA Results'</code> )           \u2013            <p>Name of the website (default: \"ISPyPSA Results\")</p> </li> <li> <code>output_filename</code>               (<code>str</code>, default:                   <code>'results_viewer.html'</code> )           \u2013            <p>Name of the output HTML file (default: \"results_viewer.html\")</p> </li> <li> <code>subtitle</code>               (<code>str</code>, default:                   <code>'Capacity Expansion Analysis'</code> )           \u2013            <p>Subtitle to display in the header (default: \"Capacity Expansion Analysis\")</p> </li> <li> <code>regions_and_zones_mapping</code>               (<code>Optional[DataFrame]</code>, default:                   <code>None</code> )           \u2013            <p>Optional mapping table to ensure correct capitalization of region and zone IDs</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None. The website is saved as output_filename in output_dir</p> </li> </ul> Source code in <code>src\\ispypsa\\plotting\\website.py</code> <pre><code>def generate_results_website(\n    plots: Dict[Path, dict],\n    plots_dir: Path,\n    output_dir: Path,\n    site_name: str = \"ISPyPSA Results\",\n    output_filename: str = \"results_viewer.html\",\n    subtitle: str = \"Capacity Expansion Analysis\",\n    regions_and_zones_mapping: Optional[pd.DataFrame] = None,\n) -&gt; None:\n    \"\"\"Generate a static website for navigating ISPyPSA plot results.\n\n    Creates a single HTML file with a navigation pane that mirrors\n    the directory structure of the plots.\n\n    Examples:\n        Perform required imports.\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; from ispypsa.data_fetch import read_csvs\n        &gt;&gt;&gt; from ispypsa.results import extract_tabular_results\n        &gt;&gt;&gt; from ispypsa.plotting import (\n        ...     create_plot_suite,\n        ...     save_plots,\n        ...     generate_results_website\n        ... )\n\n        Extract results and create plots.\n        &gt;&gt;&gt; ispypsa_tables = read_csvs(Path(\"ispypsa_inputs\"))\n        &gt;&gt;&gt; results = extract_tabular_results(network, ispypsa_tables)\n        &gt;&gt;&gt; plots = create_plot_suite(results)\n\n        Save plots to disk first.\n        &gt;&gt;&gt; plots_dir = Path(\"outputs/capacity_expansion_plots\")\n        &gt;&gt;&gt; save_plots(plots, plots_dir)\n\n        Generate the results website.\n        &gt;&gt;&gt; generate_results_website(\n        ...     plots,\n        ...     plots_dir,\n        ...     output_dir=Path(\"outputs\"),\n        ...     output_filename=\"capacity_expansion_results_viewer.html\",\n        ...     subtitle=\"Capacity Expansion Analysis\",\n        ...     regions_and_zones_mapping=results[\"regions_and_zones_mapping\"]\n        ... )\n        # Creates outputs/capacity_expansion_results_viewer.html\n\n    Args:\n        plots: Dictionary with Path keys (plot file paths) and dict values\n               containing \"plot\" and \"data\" keys (output from create_*_plot_suite)\n        plots_dir: Directory where plots are saved (e.g., outputs/capacity_expansion_plots)\n        output_dir: Directory where the website HTML should be saved (e.g., outputs directory)\n        site_name: Name of the website (default: \"ISPyPSA Results\")\n        output_filename: Name of the output HTML file (default: \"results_viewer.html\")\n        subtitle: Subtitle to display in the header (default: \"Capacity Expansion Analysis\")\n        regions_and_zones_mapping: Optional mapping table to ensure correct capitalization\n            of region and zone IDs\n\n    Returns:\n        None. The website is saved as output_filename in output_dir\n    \"\"\"\n    logging.info(f\"Generating results website: {output_filename}...\")\n\n    # Get list of plot paths\n    plot_paths = list(plots.keys())\n\n    if not plot_paths:\n        logging.warning(\"No plots found to generate website\")\n        return\n\n    # Derive plot directory name from the plots_dir path\n    plot_dir_name = plots_dir.name\n\n    # Build plot tree structure\n    plot_tree = _build_plot_tree(plot_paths)\n\n    # Build known IDs list if mapping is provided\n    known_ids_list = []\n    if regions_and_zones_mapping is not None:\n        known_ids = set()\n        for col in [\"nem_region_id\", \"isp_sub_region_id\", \"rez_id\"]:\n            if col in regions_and_zones_mapping.columns:\n                ids = regions_and_zones_mapping[col].dropna().astype(str).unique()\n                for id_val in ids:\n                    # Standard version with spaces (matches text where underscores were replaced)\n                    known_ids.add(id_val.replace(\"_\", \" \").upper())\n                    # Hyphenated version (matches text with hyphens, e.g. SEQ-1)\n                    known_ids.add(id_val.replace(\"_\", \"-\").upper())\n\n        # Add always capitalized words\n        known_ids.update(ALWAYS_CAPITALIZED_WORDS)\n\n        # Convert to sorted list (by length descending to handle overlapping IDs correctly)\n        known_ids_list = sorted(known_ids, key=len, reverse=True)\n    else:\n        # If no mapping provided, at least use the hardcoded list\n        known_ids_list = sorted(ALWAYS_CAPITALIZED_WORDS, key=len, reverse=True)\n\n    # Convert tree to HTML\n    plot_tree_html = _tree_to_html(plot_tree, known_ids=known_ids_list)\n\n    # Generate complete HTML\n    html_content = _generate_html_template(\n        plot_tree_html, plot_dir_name, site_name, subtitle\n    )\n\n    # Write HTML file\n    output_file = output_dir / output_filename\n    output_file.write_text(html_content, encoding=\"utf-8\")\n\n    logging.info(f\"Website generated successfully: {output_file}\")\n    logging.info(f\"Open {output_file.name} in a browser to view the results\")\n</code></pre>"},{"location":"cli/","title":"ISPyPSA Command Line Interface","text":"<p>The <code>ispypsa</code> command provides a user-friendly interface for running ISPyPSA workflows. It's the simplest and quickest way to run the predefined workflows. However, if more flexibility is required the API might be a better option.</p>"},{"location":"cli/#overview","title":"Overview","text":"<p>The <code>ispypsa</code> command and sub-tasks allow you to:</p> <ul> <li>Download data</li> <li>Extract data from ISP workbooks</li> <li>Generate model input files</li> <li>Run capacity expansion and operational models</li> <li>Manage workflow outputs</li> </ul> <p>Most tasks require a configuration file that specifies paths and model parameters. See the Workflow overview section for a high-level overview of the default ISPyPSA workflow.</p>"},{"location":"cli/#installation","title":"Installation","text":"<p>See the Installation section in the Getting Started guide.</p>"},{"location":"cli/#basic-usage","title":"Basic Usage","text":"<p>The basic command structure is:</p> uvplain python <pre><code>uv run ispypsa config=&lt;config_file&gt; [task]\n</code></pre> <pre><code>ispypsa config=&lt;config_file&gt; [task]\n</code></pre> <p>The <code>config=&lt;config_file&gt;</code> argument is required for task execution and must point to a valid ISPyPSA configuration YAML file.</p> <p>Important</p> <p>For the sake of brevity the commands on the rest of this page are given in plain Python, but can be run with <code>uv run</code> as well.</p>"},{"location":"cli/#examples","title":"Examples","text":"<pre><code># List available tasks (no config required)\nispypsa list\n\n# Run a specific task with config\nispypsa config=my_config.yaml create_and_run_capacity_expansion_model\n\n# Run all tasks with config\nispypsa config=my_config.yaml\n</code></pre>"},{"location":"cli/#tasks","title":"Tasks","text":"<p>The ISPyPSA CLI workflow consists of a series of tasks, each dependent on the outputs of previous tasks. The following diagram shows the task dependency graph:</p> <pre><code>save_config\n\u2502\n\u2514\u2500\u2500&gt; cache_required_iasr_workbook_tables\n     \u2502\n     \u2514\u2500\u2500&gt; create_ispypsa_inputs\n          \u2502\n          \u251c\u2500\u2500&gt; create_pypsa_friendly_inputs\n          \u2502    \u2502\n          \u2502    \u251c\u2500\u2500&gt; create_and_run_capacity_expansion_model\n          \u2502    \u2502    \u2502\n          \u2514\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u253c\u2500\u2500&gt; create_operational_timeseries\n               \u2502    \u2502    \u2502\n               \u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500&gt; create_and_run_operational_model\n</code></pre> <p>Important</p> <p>Each task depends on the outputs of previous tasks. If a particular task is run, but the previous tasks' runs on which it depends isn't up to date, then the CLI will detect this and also run the previous tasks. The detection of a previous task's being 'up to date' is based on two checks 1) its input files haven't been modified since it last ran and 2) its output files exist. If either 1) or 2) aren't true then a task is not up to date and will be rerun. This applies to both the primary target task and all of its dependencies.</p> <p>Note</p> <p>ISPyPSA provides independent download and plotting tasks. These tasks are not part of the main workflow task dependency and need be explicity run when needed.</p>"},{"location":"cli/#download_workbook","title":"download_workbook","text":"<p>Downloads the ISP workbook Excel file from the data repository.</p> <p>Usage with config file:</p> <pre><code>ispypsa config=config.yaml download_workbook\n</code></pre> <p>Usage with direct parameters (no config file needed):</p> <pre><code>ispypsa workbook_version=6.0 workbook_path=data/workbooks/6.0.xlsx download_workbook\n</code></pre> <p>Parameters (if not using config):</p> <ul> <li><code>workbook_version</code> (required): Version of workbook to download (e.g., \"6.0\")</li> <li><code>workbook_path</code> (required): Full path where workbook should be saved (must end with .xlsx)</li> </ul> <p>Outputs:</p> <ul> <li>Excel workbook file at the specified path</li> </ul> <p>Notes:</p> <ul> <li>This task will overwrite any existing workbook file at the target location</li> <li>If download fails partway through, partial files will remain</li> <li>Task always runs when invoked (never considers itself up-to-date)</li> </ul>"},{"location":"cli/#download_trace_data","title":"download_trace_data","text":"<p>Downloads trace data (demand, wind, solar) from the data repository.</p> <p>Usage with config file:</p> <pre><code># Download example dataset (default - smaller, for testing)\nispypsa config=config.yaml download_trace_data\n\n# Download full dataset (override config default)\nispypsa config=config.yaml trace_dataset_type=full download_trace_data\n</code></pre> <p>Usage with direct parameters (no config file needed):</p> <pre><code># Download example dataset to specified directory\nispypsa save_directory=data/traces download_trace_data\n\n# Download full dataset to specified directory\nispypsa save_directory=data/traces trace_dataset_type=full download_trace_data\n\n# Specify all parameters\nispypsa save_directory=data/traces trace_dataset_type=example trace_dataset_year=2024 download_trace_data\n</code></pre> <p>Parameters (if not using config):</p> <ul> <li><code>save_directory</code>: Directory where trace data should be saved</li> <li><code>trace_dataset_type</code> (optional): Either \"example\" or \"full\"<ul> <li>Defaults to \"example\"</li> <li>\"example\": Smaller dataset suitable for testing and development</li> <li>\"full\": Complete dataset for production runs</li> </ul> </li> <li><code>trace_dataset_year</code> (optional): Year of dataset<ul> <li>Defaults to 2024</li> <li>Currently only 2024 is supported</li> </ul> </li> </ul> <p>Outputs:</p> <ul> <li>Trace data files in the specified directory</li> <li>Files organized in subdirectories: <code>isp_{year}/project/</code>, <code>isp_{year}/zone/</code>, <code>isp_{year}/demand/</code></li> </ul> <p>Notes:</p> <ul> <li>This task will re-download all files, overwriting any existing trace data</li> <li>If download fails partway through, partial files will remain</li> <li>Task always runs when invoked (never considers itself up-to-date)</li> <li>The \"full\" dataset manifest is currently empty and will be populated in future releases</li> <li>Direct mode is useful for downloading data before creating a config file</li> </ul>"},{"location":"cli/#cache_required_iasr_workbook_tables","title":"cache_required_iasr_workbook_tables","text":"<p>Extracts data from the ISP Excel workbook and caches it as CSV files.</p> <pre><code>ispypsa config=config.yaml cache_required_iasr_workbook_tables\n</code></pre> <p>Inputs:</p> <ul> <li>IASR Excel workbook (specified in config <code>paths.workbook_path</code>)</li> </ul> <p>Outputs:</p> <ul> <li>CSV files in the workbook cache directory (location specified in config <code>paths.   parsed_workbook_cache</code>)</li> </ul>"},{"location":"cli/#create_ispypsa_inputs","title":"create_ispypsa_inputs","text":"<p>Generates ISPyPSA format input tables from the cached workbook data.</p> <pre><code>ispypsa config=config.yaml create_ispypsa_inputs\n</code></pre> <p>Inputs:</p> <ul> <li>Cached workbook CSV files (location specified in config <code>paths.   parsed_workbook_cache</code>)</li> </ul> <p>Outputs:</p> <ul> <li>ISPyPSA input tables in <code>{run_directory}/{ispypsa_run_name}/ispypsa_inputs/</code>   (run_directory and ispypsa_run_name specified in config)</li> </ul>"},{"location":"cli/#create_pypsa_friendly_inputs","title":"create_pypsa_friendly_inputs","text":"<p>Converts ISPyPSA format tables to PyPSA-compatible format and generates time series data for capacity expansion.</p> <pre><code>ispypsa config=config.yaml create_pypsa_friendly_inputs\n</code></pre> <p>Inputs:</p> <ul> <li>ISPyPSA input tables in <code>{run_directory}/{ispypsa_run_name}/ispypsa_inputs/</code>   (run_directory and ispypsa_run_name specified in config)</li> <li>Trace data (demand, wind, solar) (location specified in config <code>paths.   parsed_traces_directory</code>)</li> </ul> <p>Outputs:</p> <ul> <li>PyPSA-friendly tables in <code>{run_directory}/{ispypsa_run_name}/pypsa_friendly/</code>   (run_directory and ispypsa_run_name specified in config)</li> <li>Time series data in <code>{run_directory}/{ispypsa_run_name}/pypsa_friendly/capacity_expansion_timeseries/</code>   (run_directory and ispypsa_run_name specified in config)</li> </ul>"},{"location":"cli/#create_and_run_capacity_expansion_model","title":"create_and_run_capacity_expansion_model","text":"<p>Creates the PyPSA network object and runs the capacity expansion optimization.</p> <pre><code>ispypsa config=config.yaml create_and_run_capacity_expansion_model\n</code></pre> <p>Inputs:</p> <ul> <li>PyPSA-friendly tables in <code>{run_directory}/{ispypsa_run_name}/pypsa_friendly/</code>   (run_directory and ispypsa_run_name specified in config)</li> <li>Time series data in <code>{run_directory}/{ispypsa_run_name}/pypsa_friendly/capacity_expansion_timeseries/</code>   (run_directory and ispypsa_run_name specified in config)</li> </ul> <p>Outputs:</p> <ul> <li>Optimized capacity expansion results in <code>{run_directory}/{ispypsa_run_name}/outputs/capacity_expansion.nc</code></li> </ul> <p>Plotting Option:</p> <p>By default, a suite of plots is automatically generated after the capacity expansion model runs. This behaviour is controlled by the <code>create_plots</code> setting in <code>config.yaml</code>, which is set to <code>True</code> in the default workflow.</p> <p>When enabled, the task will create and save a suite of plots to the <code>{run_directory}/{ispypsa_run_name}/outputs/capacity_expansion_plots/</code> directory.</p> <p>This provides a convenient way to visualise model outputs without needing to run the <code>create_capacity_expansion_plots</code> task separately.</p> <p>You can disable plot creation by setting <code>create_plots: False</code> in your config or by passing it as a command-line argument:</p> <pre><code># Disable plot generation for this run\nispypsa config=config.yaml create_plots=False create_and_run_capacity_expansion_model\n</code></pre> <p>Skip Optimization Option:</p> <p>You can skip the optimization step and only build the network using the <code>run_optimisation</code> flag:</p> <pre><code>ispypsa config=config.yaml run_optimisation=False create_and_run_capacity_expansion_model\n</code></pre> <p>This is particularly useful for:</p> <ul> <li>Testing that your model configuration is valid</li> <li>Verifying network construction without waiting for optimization</li> <li>Debugging model setup issues</li> <li>Creating a network file for manual inspection or custom optimization</li> </ul> <p>When this flag is set to <code>False</code>, the task will:</p> <ol> <li>Build the complete PyPSA network with all components and constraints</li> <li>Save the unoptimized network to the output file</li> <li>Skip the potentially time-consuming optimization step</li> </ol>"},{"location":"cli/#create_capacity_expansion_plots","title":"create_capacity_expansion_plots","text":"<p>Generates plots from the capacity expansion model results.</p> <pre><code>ispypsa config=config.yaml create_capacity_expansion_plots\n</code></pre> <p>Inputs:</p> <ul> <li>Tabular results from capacity expansion model in <code>{run_directory}/{ispypsa_run_name}/outputs/capacity_expansion_tables/</code> (generated by <code>create_and_run_capacity_expansion_model</code>)</li> </ul> <p>Outputs:</p> <ul> <li>Plot files (interactive HTML format) in <code>{run_directory}/{ispypsa_run_name}/outputs/capacity_expansion_plots/</code></li> <li>Results viewer website: <code>{run_directory}/{ispypsa_run_name}/outputs/capacity_expansion_results_viewer.html</code></li> </ul> <p>Notes:</p> <ul> <li>This task is not part of the main workflow dependency chain and must be run explicitly.</li> <li>It will fail if the capacity expansion results are not present.</li> <li>Generated plots include:</li> <li>Regional and sub-regional dispatch plots</li> <li>Transmission flow plots for all links</li> <li>Transmission capacity expansion plots</li> <li>Regional transmission expansion plots</li> </ul>"},{"location":"cli/#create_operational_timeseries","title":"create_operational_timeseries","text":"<p>Creates time series data for operational modeling.</p> <pre><code>ispypsa config=config.yaml create_operational_timeseries\n</code></pre> <p>Inputs:</p> <ul> <li>ISPyPSA input tables in <code>{run_directory}/{ispypsa_run_name}/ispypsa_inputs/</code>   (run_directory and ispypsa_run_name specified in config)</li> <li>Trace data (demand, wind, solar) (location specified in config <code>paths.   parsed_traces_directory</code>)</li> </ul> <p>Outputs:</p> <ul> <li>Operational time series data in <code>{run_directory}/{ispypsa_run_name}/pypsa_friendly/operational_timeseries/</code></li> </ul>"},{"location":"cli/#create_and_run_operational_model","title":"create_and_run_operational_model","text":"<p>Prepares the PyPSA network object for operational modeling using fixed capacities from capacity expansion and runs the operational optimization.</p> <pre><code>ispypsa config=config.yaml create_and_run_operational_model\n</code></pre> <p>Inputs:</p> <ul> <li>Capacity expansion results in <code>{run_directory}/{ispypsa_run_name}/outputs/capacity_expansion.nc</code></li> <li>PyPSA-friendly tables in <code>{run_directory}/{ispypsa_run_name}/pypsa_friendly/</code></li> <li>Operational time series data in <code>{run_directory}/{ispypsa_run_name}/pypsa_friendly/operational_timeseries/</code></li> </ul> <p>Outputs:</p> <ul> <li>Operational optimization results in <code>{run_directory}/{ispypsa_run_name}/outputs/operational.nc</code></li> </ul> <p>Running Without Capacity Expansion</p> <p>The operational model can be built and run even if the capacity expansion optimization was skipped (using <code>run_optimisation=False</code>). However, this will significantly affect the operational model outputs because:</p> <ul> <li>The network will use initial capacities rather than optimized capacities</li> <li>No new generation or transmission capacity will have been added</li> <li>The operational model may be infeasible if initial capacities are insufficient</li> <li>Results will not reflect least-cost capacity investment decisions</li> </ul> <p>Skip Optimization Option:</p> <p>You can skip the optimization step and only prepare the network using the <code>run_optimisation</code> flag:</p> <pre><code>ispypsa config=config.yaml run_optimisation=False create_and_run_operational_model\n</code></pre> <p>This is particularly useful for:</p> <ul> <li>Testing operational model setup without running the full optimization</li> <li>Verifying that capacity expansion results load correctly</li> <li>Debugging time series data integration</li> <li>Creating a prepared network for custom operational analysis</li> </ul> <p>When this flag is set to <code>False</code>, the task will:</p> <ol> <li>Load the capacity expansion results</li> <li>Update the network with operational time series data</li> <li>Fix the optimal capacities from capacity expansion</li> <li>Save the prepared network without running the rolling horizon optimization</li> <li>Skip the potentially very long operational optimization process</li> </ol> <p>Plotting Option:</p> <p>Similar to capacity expansion, plots can be automatically generated after the operational model runs. This behaviour is controlled by the <code>create_plots</code> setting in <code>config.yaml</code>, which defaults to <code>True</code>.</p> <p>When enabled, the task will create and save a suite of plots to the <code>{run_directory}/{ispypsa_run_name}/outputs/operational_plots/</code> directory.</p> <p>You can disable plot creation by setting <code>create_plots: False</code> in your config or by passing it as a command-line argument:</p> <pre><code># Disable plot generation for this run\nispypsa config=config.yaml create_plots=False create_and_run_operational_model\n</code></pre>"},{"location":"cli/#create_operational_plots","title":"create_operational_plots","text":"<p>Generates plots from the operational model results.</p> <pre><code>ispypsa config=config.yaml create_operational_plots\n</code></pre> <p>Inputs:</p> <ul> <li>Tabular results from operational model in <code>{run_directory}/{ispypsa_run_name}/outputs/operational_tables/</code> (generated by <code>create_and_run_operational_model</code>)</li> </ul> <p>Outputs:</p> <ul> <li>Plot files (interactive HTML format) in <code>{run_directory}/{ispypsa_run_name}/outputs/operational_plots/</code></li> <li>Results viewer website: <code>{run_directory}/{ispypsa_run_name}/outputs/operational_results_viewer.html</code></li> </ul> <p>Notes:</p> <ul> <li>This task is not part of the main workflow dependency chain and must be run explicitly.</li> <li>It will fail if the operational results are not present.</li> <li>Generated plots include:</li> <li>Regional and sub-regional dispatch plots</li> <li>Transmission flow plots for all links</li> </ul>"},{"location":"cli/#list","title":"list","text":"<p>Shows all available tasks and their status. No config file required.</p> <pre><code>ispypsa list\n</code></pre>"},{"location":"cli/#save_config","title":"save_config","text":"<p>Saves a copy of the configuration file to the run directory. This task runs automatically as a dependency of other tasks to preserve the exact configuration used for reproducibility.</p> <pre><code>ispypsa config=config.yaml save_config\n</code></pre> <p>Inputs:</p> <ul> <li>Configuration YAML file (specified with <code>config=</code>)</li> </ul> <p>Outputs:</p> <ul> <li>Copy of the configuration file in <code>{run_directory}/{ispypsa_run_name}/</code>   (preserves the exact config used for the run)</li> </ul> <p>Note: This task always runs (never considers itself up-to-date) to ensure the config file is always current.</p>"},{"location":"cli/#configuration","title":"Configuration","text":"<p>The <code>config=</code> argument is required for most tasks. It accepts either absolute or relative paths:</p> <pre><code># Absolute path\nispypsa config=/home/user/projects/my_config.yaml task_name\n\n# Relative path (from current directory)\nispypsa config=../configs/my_config.yaml task_name\n\n# File in current directory\nispypsa config=my_config.yaml task_name\n</code></pre> <p>See the example configuration file for the required format.</p>"},{"location":"cli/#examples_1","title":"Examples","text":""},{"location":"cli/#complete-workflow","title":"Complete Workflow","text":"<p>Run all tasks one by one to generate model results:</p> <pre><code># Extract workbook data\nispypsa config=config.yaml cache_required_iasr_workbook_tables\n\n# Generate ISPyPSA inputs\nispypsa config=config.yaml create_ispypsa_inputs\n\n# At this stage the ISPyPSA inputs could be edited to adjust build costs or any other\n# inputs set out in {run_directory}/{ispypsa_run_name}/ispypsa_inputs/\n\n# Convert to PyPSA format and run capacity expansion\nispypsa config=config.yaml create_and_run_capacity_expansion_model\n\n# Create operational time series\nispypsa config=config.yaml create_operational_timeseries\n\n# Run operational optimization\nispypsa config=config.yaml create_and_run_operational_model\n\n# Generate plots (optional - also runs automatically if create_plots=True)\nispypsa config=config.yaml create_capacity_expansion_plots\nispypsa config=config.yaml create_operational_plots\n</code></pre>"},{"location":"cli/#simplified-complete-workflow","title":"Simplified Complete Workflow","text":"<p>The complete workflow can be run in fewer steps, with intermediate tasks run automatically when their outputs are required.</p> <pre><code># Generate ISPyPSA inputs\nispypsa config=config.yaml create_ispypsa_inputs\n\n# At this stage the ISPyPSA inputs could be edited to adjust build cost or any other\n# inputs set out in {run_directory}/{ispypsa_run_name}/ispypsa_inputs/\n\n# Run complete workflow (all remaining tasks)\nispypsa config=config.yaml create_and_run_operational_model\n</code></pre>"},{"location":"cli/#running-from-different-directories","title":"Running from Different Directories","text":"<p>The CLI works correctly from any directory and handles relative paths appropriately:</p> <pre><code># From project root - no config needed for list\nispypsa list\n\n# From project root - config for task execution\nispypsa config=ispypsa_config.yaml create_ispypsa_inputs\n\n# From a subdirectory\ncd analysis\nispypsa config=../ispypsa_config.yaml create_ispypsa_inputs\n\n# Using an absolute path (works from anywhere)\nispypsa config=/home/user/ispypsa/config.yaml create_ispypsa_inputs\n</code></pre>"},{"location":"cli/#using-different-configurations","title":"Using Different Configurations","text":"<p>You can easily switch between different model configurations:</p> <pre><code># Development configuration\nispypsa config=configs/dev_config.yaml create_and_run_capacity_expansion_model\n\n# Production configuration with different parameters\nispypsa config=configs/prod_config.yaml create_and_run_capacity_expansion_model\n\n# Test configuration with smaller dataset\nispypsa config=configs/test_config.yaml create_and_run_capacity_expansion_model\n</code></pre>"},{"location":"cli/#skip-optimization-flags","title":"Skip Optimization Flags","text":"<p>You can skip the optimization step in modeling tasks using these flags:</p> <pre><code># Skip capacity expansion optimization (only prepare the network)\nispypsa config=config.yaml run_optimisation=False create_and_run_capacity_expansion_model\n\n# Skip operational optimization (only prepare the network, capacity expansion will also be skipped)\nispypsa config=config.yaml run_optimisation=False create_and_run_operational_model\n\n# Run both optimizations normally (default behavior)\nispypsa config=config.yaml run_optimisation=True create_and_run_operational_model\n</code></pre> <p>These flags are useful for:</p> <ul> <li>Debugging network construction without waiting for optimization</li> <li>Testing model setup and configuration</li> <li>Preparing networks for manual optimization or analysis</li> </ul>"},{"location":"cli/#path-issues","title":"Path Issues","text":"<p>If you encounter path-related errors:</p> <ol> <li>Use <code>debug=True</code> to see how paths are being resolved</li> <li>Try using absolute paths in your config file</li> <li>Ensure all directories exist before running tasks</li> <li>Check file permissions for input/output directories</li> </ol>"},{"location":"cli/#advanced-usage","title":"Advanced Usage","text":""},{"location":"cli/#how-ispypsa-works","title":"How ispypsa Works","text":"<p>The <code>ispypsa</code> command is built on the <code>doit</code> task automation tool with ISPyPSA-specific enhancements:</p> <ol> <li>run command: Uses doit's native run command with <code>config=value</code> parameter support</li> <li>Other commands: Uses doit's built-in commands (list, help, etc.) directly</li> <li>Lazy loading: Configuration is only loaded when tasks actually execute</li> <li>Path resolution: All paths work relative to where you run the command</li> </ol>"},{"location":"cli/#passing-additional-doit-options","title":"Passing Additional doit Options","text":"<p>You can pass any doit option to the run command:</p> <pre><code># Run with verbose output\nispypsa config=config.yaml -v create_and_run_capacity_expansion_model\n\n# Always execute task ignoring up-to-date checks\nispypsa config=config.yaml -a create_ispypsa_inputs\n\n# Continue executing even after task failure\nispypsa config=config.yaml --continue create_and_run_operational_model\n</code></pre> <p>Common doit options for the run command:</p> <ul> <li><code>-v</code> / <code>--verbosity</code>: Set verbosity level (0-2)</li> <li><code>-a</code> / <code>--always-execute</code>: Always execute tasks even if up-to-date</li> <li><code>--continue</code>: Continue executing tasks even after failure</li> <li><code>-s</code> / <code>--single</code>: Execute only specified tasks ignoring their dependencies</li> </ul>"},{"location":"cli/#commands-without-config","title":"Commands Without Config","text":"<p>Some commands work without requiring a config file:</p> <pre><code># List all available tasks\nispypsa list\n\n# Show help\nispypsa help\n\n# Show help for specific command\nispypsa help run\n</code></pre>"},{"location":"cli/#working-directory-behavior","title":"Working Directory Behavior","text":"<p>The CLI maintains your current working directory:</p> <ul> <li>All paths in config files work relative to where you run the command</li> <li>No directory changes occur during execution</li> <li>Output files are created according to paths in your config file</li> </ul>"},{"location":"config/","title":"Config","text":"<p>The config is a yaml file used to specify all the modelling settings or inputs that are not otherwise provided in the ISPyPSA input tables. Each of the settings is documented on this page and an example config file can be found here.</p>"},{"location":"config/#paths","title":"Paths","text":"<p>The paths settings specify the locations of model input and output data. Either relative or absolute paths can be used. Relative paths assume the run directory (CLI or API) is the root directory of the relative path.</p>"},{"location":"config/#pathsparsed_traces_directory","title":"paths.parsed_traces_directory","text":"<p>The path to the folder containing parsed demand, wind and solar traces.  If set to ENV the path will be retrieved from the environment variable \"PATH_TO_PARSED_TRACES\".</p> <p>Examples:</p> <p><code>parsed_traces_directory: ENV</code></p> <p><code>parsed_traces_directory: \"path/to/traces\"</code></p>"},{"location":"config/#pathsworkbook_path","title":"paths.workbook_path","text":"<p>The path to the ISP workbook Excel file.</p> <p>Examples:</p> <p><code>workbook_path: \"path/to/workbook\"</code></p>"},{"location":"config/#pathsparsed_workbook_cache","title":"paths.parsed_workbook_cache","text":"<p>The path to the workbook table cache directory.</p> <p>Examples:</p> <p><code>parsed_workbook_cache: path/to/workbook/cache</code></p>"},{"location":"config/#pathsrun_directory","title":"paths.run_directory","text":"<p>The run directory where all inputs and outputs will be stored. Subdirectories will be created automatically:</p> <ul> <li>{run_directory}/{ispypsa_run_name}/ispypsa_inputs</li> <li>{run_directory}/{ispypsa_run_name}/pypsa_friendly</li> <li>{run_directory}/{ispypsa_run_name}/pypsa_friendly/capacity_expansion_timeseries</li> <li>{run_directory}/{ispypsa_run_name}/pypsa_friendly/operational_timeseries</li> <li>{run_directory}/{ispypsa_run_name}/outputs</li> </ul> <p>Examples:</p> <p><code>run_directory: \"path/to/directory\"</code></p>"},{"location":"config/#pathsispypsa_run_name","title":"paths.ispypsa_run_name","text":"<p>The name of the ISPyPSA model run. This name is used to select the output folder within <code>run_directory</code>.</p> <p>Examples:</p> <p><code>ispypsa_run_name: modelling_test_run</code></p>"},{"location":"config/#trace-data","title":"Trace Data","text":""},{"location":"config/#trace_datadataset_type","title":"trace_data.dataset_type","text":"<p>The type of trace dataset to download when using CLI download tasks. This setting is only used by the <code>download_trace_data</code> CLI task.</p> <p>Options:</p> <ul> <li>\"example\": Smaller dataset suitable for testing and development</li> <li>\"full\": Complete dataset for production runs</li> </ul> <p>Default: \"example\"</p> <p>Examples:</p> <p><code>dataset_type: example</code></p>"},{"location":"config/#trace_datadataset_year","title":"trace_data.dataset_year","text":"<p>The year of trace dataset to download when using CLI download tasks. This setting is used by the <code>download_trace_data</code> CLI task and by other modelling tasks to select the correct data set from the trace data directory. Currently only 2024 is supported.</p> <p>Default: 2024</p> <p>Examples:</p> <p><code>dataset_year: 2024</code></p>"},{"location":"config/#ispypsa-templating","title":"ISPyPSA Templating","text":""},{"location":"config/#iasr_workbook_version","title":"iasr_workbook_version","text":"<p>The version of IASR workbook that the template inputs are generated from. The workbook version is used to retrieve IASR data which has been manually extracted (rather than via isp-workbook-parser) and packaged with ISPyPSA. Some data needs to be manually extracted because of the formatting of the IASR workbook.</p> <p>Examples:</p> <p><code>iasr_workbook_version: \"6.0\"</code></p>"},{"location":"config/#scenario","title":"scenario","text":"<p>The ISP scenario for which to generate ISPyPSA inputs.</p> <p>Options (descriptions lifted from the 2024 ISP):</p> <ul> <li>\"Progressive Change\": Reflects slower economic growth and energy investment with economic and international factors placing industrial demands at greater risk and slower decarbonisation action beyond current commitments</li> <li>\"Step Change\": Fulfils Australia's emission reduction commitments in a growing economy</li> <li>\"Green Energy Exports\": Sees very strong industrial decarbonisation and low-emission energy exports</li> </ul> <p>Examples:</p> <p><code>scenario: Step Change</code></p>"},{"location":"config/#financialeconomic","title":"Financial/Economic","text":""},{"location":"config/#wacc","title":"wacc","text":"<p>Weighted average cost of capital for annuitisation of generation and transmission costs, as a fraction, i.e. 0.07 is 7%.</p> <p>Examples:</p> <p><code>wacc: 0.07</code></p>"},{"location":"config/#discount_rate","title":"discount_rate","text":"<p>Discount rate applied to model objective function, as a fraction, i.e. 0.05 is 5%.</p> <p>Examples:</p> <p><code>discount_rate: 0.05</code></p>"},{"location":"config/#unserved-energy","title":"Unserved Energy","text":"<p>Unserved energy can be allowed at each node in the network with energy demand to prevent model infeasibility.</p>"},{"location":"config/#unserved_energycost","title":"unserved_energy.cost","text":"<p>Cost of unserved energy in $/MWh.</p> <p>Set to 'None' to disable unserved energy generators.</p> <p>Examples:</p> <p><code>cost: 10000.0</code></p>"},{"location":"config/#unserved_energymax_per_node","title":"unserved_energy.max_per_node","text":"<p>Maximum allowed unserved demand MW per demand network node. Defaults to 1e5 (100,000 MW). Larger values may cause problems for optimisation solvers.</p> <p>Examples:</p> <p><code>max_per_node: 100000.0</code></p>"},{"location":"config/#network","title":"Network","text":""},{"location":"config/#networktransmission_expansion","title":"network.transmission_expansion","text":"<p>Does the model consider the expansion of sub-region to sub-region transmission capacity.</p> <p>Examples:</p> <p><code>transmission_expansion: True</code></p>"},{"location":"config/#networkrez_transmission_expansion","title":"network.rez_transmission_expansion","text":"<p>Does the model consider the expansion of renewable energy zone transmission capacity.</p> <p>Examples:</p> <p><code>rez_transmission_expansion: True</code></p>"},{"location":"config/#networkannuitisation_lifetime","title":"network.annuitisation_lifetime","text":"<p>Years to annuitise transmission project capital costs over.</p> <p>Examples:</p> <p><code>annuitisation_lifetime: 30</code></p>"},{"location":"config/#networknodesregional_granularity","title":"network.nodes.regional_granularity","text":"<p>The regional granularity of the nodes in the modelled network. Only \"sub_regions\" is implemented and allowed currently.</p> <p>Options:</p> <ul> <li>\"sub_regions\": ISP sub-regions are added as network nodes (12 nodes)</li> <li>\"nem_regions\": NEM regions are added as network nodes (5 nodes)</li> <li>\"single_region\": A single node, the Victorian sub-region, is added as a network node (1 node)</li> </ul> <p>Examples:</p> <p><code>regional_granularity: sub_regions</code></p>"},{"location":"config/#networknodesrezs","title":"network.nodes.rezs","text":"<p>Whether Renewable Energy Zones (REZs) are modelled as distinct nodes.</p> <p>Options:</p> <ul> <li>\"discrete_nodes\": REZs are added as network nodes to model REZ transmission limits</li> <li>\"attached_to_parent_node\": REZ resources are attached to their parent node (sub-region or NEM region)</li> </ul> <p>Examples:</p> <p><code>rezs: discrete_nodes</code></p>"},{"location":"config/#networkrez_to_sub_region_transmission_default_limit","title":"network.rez_to_sub_region_transmission_default_limit","text":"<p>Link capacity limit for rez to node connections that have their limit's modelled through custom constraint (MW).</p> <p>The export limits for some REZs are modelled via custom constraints which incorporate multiple transmission line flows and/or generator dispatch levels. For these REZs a PyPSA link object between the REZ and sub region node is still created but with a very high capacity limit such the that the custom constraint always sets the actual REZ export limit.</p> <p>Examples:</p> <p><code>rez_to_sub_region_transmission_default_limit: 1e5</code></p>"},{"location":"config/#temporal","title":"Temporal","text":""},{"location":"config/#temporalyear_type","title":"temporal.year_type","text":"<p>Whether the model uses financial (\"fy\") or calendar (\"calendar\") years.</p> <p>Examples:</p> <p><code>year_type: fy</code></p>"},{"location":"config/#temporalrangestart_year","title":"temporal.range.start_year","text":"<p>Model begins at the start of the start year. E.g. the first time interval for a financial year model starting in 2025 would be 2024-07-01 00:30:00.</p> <p>Examples:</p> <p><code>start_year: 2025</code></p>"},{"location":"config/#temporalrangeend_year","title":"temporal.range.end_year","text":"<p>Model ends at the end of the end year. E.g. the last time interval for a financial year model ending in 2028 would be 2028-07-01 00:00:00.</p> <p>Examples:</p> <p><code>end_year: 2035</code></p>"},{"location":"config/#temporalcapacity_expansion","title":"temporal.capacity_expansion","text":"<p>The temporal settings for the capacity expansion phase of the modelling.</p>"},{"location":"config/#temporalcapacity_expansionresolution_min","title":"temporal.capacity_expansion.resolution_min","text":"<p>The temporal resolution in minutes. Currently, only the 30 min resolution is implemented/allowed. Value should be provided as an integer.</p> <p>Examples:</p> <p><code>resolution_min: 30</code></p>"},{"location":"config/#temporalcapacity_expansionreference_year_cycle","title":"temporal.capacity_expansion.reference_year_cycle","text":"<p>The order in which different weather reference years are used in the model. If the number of reference years is less than the number of model years the reference years will be reused in the order provided. For example, if there are 5 model years and 2 references years, [A, B], are given in reference_year_cycle, the reference years are used in the order [A, B, A, B, A].</p> <p>Examples:</p> <p><code>reference_year_cycle: [2018, 2021, 2023]</code></p>"},{"location":"config/#temporalcapacity_expansioninvestment_periods","title":"temporal.capacity_expansion.investment_periods","text":"<p>List of investment period start years. An investment period runs from the beginning of the year (financial or calendar) until the next the period begins or the model end year is reached.</p> <p>Examples:</p> <p><code>investment_periods: [2025, 2030]</code></p>"},{"location":"config/#temporalcapacity_expansionaggregationrepresentative_weeks","title":"temporal.capacity_expansion.aggregation.representative_weeks","text":"<p>Representative weeks to use instead of full yearly temporal representation.</p> <p>Options:</p> <ul> <li>\"None\": Full yearly temporal representation is used or another aggregation.</li> <li>list[int]: a list of integers specifying weeks of year to use as representative. Weeks   of year are defined as full weeks (Monday-Sunday) falling within the year. For   example, if the list is \"[1]\" the model will only use the first full week of each   modelled year.</li> </ul> <p>Examples:</p> <p><code>representative_weeks: [12, 25, 40]</code></p>"},{"location":"config/#temporalcapacity_expansionaggregationnamed_representative_weeks","title":"temporal.capacity_expansion.aggregation.named_representative_weeks","text":"<p>Named representative weeks to use instead of full yearly temporal representation.</p> <p>Options:</p> <ul> <li>\"None\": Full yearly temporal representation is used or another aggregation.</li> <li>list[str]: A list of strings from the following options: peak-demand, residual-peak-demand, minimum-demand,    residual-minimum-demand, peak-consumption, residual-peak-consumption. Only weeks which fall fully within a model    calendar or financial year are considered for selection.</li> </ul> <p>Examples:</p> <p><code>named_representative_weeks: [residual-peak-demand, minimum-demand]</code></p>"},{"location":"config/#temporaloperational","title":"temporal.operational","text":"<p>The temporal settings for the operational phase of the modelling.</p>"},{"location":"config/#temporaloperationalresolution_min","title":"temporal.operational.resolution_min","text":"<p>The temporal resolution in minutes. Currently, only the 30 min resolution is implemented/allowed. Value should be provided as an integer.</p> <p>Examples:</p> <p><code>resolution_min: 30</code></p>"},{"location":"config/#temporaloperationalreference_year_cycle","title":"temporal.operational.reference_year_cycle","text":"<p>The order in which different weather reference years are used in the model. If the number of reference years is less than the number of model years the reference years will be reused in the order provided. For example, if there are 5 model years and 2 references years, [A, B], are given in reference_year_cycle, the reference years are used in the order [A, B, A, B, A].</p> <p>Examples:</p> <p><code>reference_year_cycle: [2018, 2021, 2023]</code></p>"},{"location":"config/#temporaloperationalhorizon","title":"temporal.operational.horizon","text":"<p>The number of time intervals to optimise over per iteration of the operational rolling horizon optimisation.</p> <p>Examples:</p> <p><code>horizon: 96</code></p>"},{"location":"config/#temporaloperationaloverlap","title":"temporal.operational.overlap","text":"<p>The number of time intervals to overlap between the current and previous iterations of the rolling horizon optimisation.</p> <p>If a horizon of 96 (48*2) and overlap of 48 was used, with a 30 min resolution, this would be equivalent to daily rolling horizon with a one day look ahead.</p> <p>Examples:</p> <p><code>overlap: 48</code></p>"},{"location":"config/#temporaloperationalaggregationrepresentative_weeks","title":"temporal.operational.aggregation.representative_weeks","text":"<p>Representative weeks to use instead of full yearly temporal representation.</p> <p>Options:</p> <ul> <li>\"None\": Full yearly temporal representation is used or another aggregation.</li> <li>list[int]: a list of integers specifying weeks of year to use as representative. Weeks   of year are defined as full weeks (Monday-Sunday) falling within the year. For   example, if the list is \"[1]\" the model will only use the first full week of each   modelled year.</li> </ul> <p>Examples:</p> <p><code>representative_weeks: [12, 25, 40]</code></p>"},{"location":"config/#temporaloperationalaggregationnamed_representative_weeks","title":"temporal.operational.aggregation.named_representative_weeks","text":"<p>Named representative weeks to use instead of full yearly temporal representation.</p> <p>Options:</p> <ul> <li>\"None\": Full yearly temporal representation is used or another aggregation.</li> <li>list[str]: A list of strings from the following options: peak-demand, residual-peak-demand, minimum-demand,    residual-minimum-demand, peak-consumption, residual-peak-consumption. Only weeks which fall fully within a model    calendar or financial year are considered for selection.</li> </ul> <p>Examples:</p> <p><code>named_representative_weeks: [residual-peak-demand, minimum-demand]</code></p>"},{"location":"config/#solver","title":"Solver","text":""},{"location":"config/#solver_1","title":"solver","text":"<p>External solver to use.</p> <p>Options (refer to https://pypsa.readthedocs.io/en/latest/getting-started/installation.html):</p> <p>Free, and by default, installed with ISPyPSA:</p> <ul> <li>\"highs\"</li> </ul> <p>Free, but must be installed by the user:</p> <ul> <li>\"cbc\"</li> <li>\"glpk\"</li> <li>\"scip\"</li> </ul> <p>Not free and must be installed by the user:</p> <ul> <li>\"cplex\"</li> <li>\"gurobi\"</li> <li>\"xpress\"</li> <li>\"mosek\"</li> <li>\"copt\"</li> <li>\"mindopt\"</li> <li>\"pips\"</li> </ul> <p>Examples:</p> <p><code>solver: highs</code></p>"},{"location":"config/#plotting","title":"Plotting","text":""},{"location":"config/#create_plots","title":"create_plots","text":"<p>Whether to automatically generate plots and a results website after the capacity expansion and operational models complete.</p> <p>Options:</p> <ul> <li>true: Enable automatic plot generation after model runs</li> <li>false: Disable automatic plot generation (plots can still be generated separately   using the <code>create_capacity_expansion_plots</code> or <code>create_operational_plots</code> CLI tasks)</li> </ul> <p>Default: false</p> <p>Examples:</p> <p><code>create_plots: true</code></p> <p>Can also be overridden on the command line:</p> <pre><code>ispypsa config=config.yaml create_plots=True create_and_run_capacity_expansion_model\n</code></pre>"},{"location":"config/#filtering","title":"Filtering","text":""},{"location":"config/#filter_by_nem_regions","title":"filter_by_nem_regions","text":"<p>Filter the model to only include specified NEM regions. If set, all NEM regions not in this list will be excluded from the model. This is useful for running smaller, faster models focused on specific regions.</p> <p>Cannot be used together with <code>filter_by_isp_sub_regions</code>.</p> <p>Default: None (all regions included)</p> <p>Examples:</p> <pre><code># Single region\nfilter_by_nem_regions: [\"NSW\"]\n\n# Multiple regions\nfilter_by_nem_regions: [\"NSW\", \"VIC\", \"QLD\"]\n</code></pre>"},{"location":"config/#filter_by_isp_sub_regions","title":"filter_by_isp_sub_regions","text":"<p>Filter the model to only include specified ISP sub-regions. If set, all sub-regions not in this list will be excluded from the model. This provides finer-grained control than <code>filter_by_nem_regions</code>.</p> <p>Cannot be used together with <code>filter_by_nem_regions</code>.</p> <p>Default: None (all sub-regions included)</p> <p>Examples:</p> <pre><code># Single sub-region\nfilter_by_isp_sub_regions: [\"CNSW\"]\n\n# Multiple sub-regions\nfilter_by_isp_sub_regions: [\"CNSW\", \"NNSW\", \"SNW\"]\n</code></pre>"},{"location":"getting_started/","title":"Getting started","text":""},{"location":"getting_started/#installation","title":"Installation","text":""},{"location":"getting_started/#new-to-python-need-help-setting-up-your-environment","title":"New to Python / need help setting up your environment?","text":"<p>Unsure?</p> <p>We recommend using the <code>uv</code> software to install and run Python for ISPyPSA.</p> <ol> <li> <p>Install <code>uv</code> by following the instructions here</p> </li> <li> <p>Using the command line install Python 3.12</p> <pre><code>uv python install 3.12\n</code></pre> </li> <li> <p>Using the command line create a new directory setup as a <code>uv</code> managed project</p> <pre><code>uv init new-ispypsa-project\n</code></pre> </li> <li> <p>In the new directory use the command line to install ISPyPSA for the project. Any additional Python packages you wish to use can be installed using the same commands.</p> <pre><code>uv add ISPyPSA\nuv sync\n</code></pre> </li> <li> <p>Any scripts you run inside the new directory using <code>uv run</code> will automatically    use the packages you've installed for the project.</p> <pre><code>uv run main.py\n</code></pre> </li> </ol>"},{"location":"getting_started/#i-know-python","title":"I know Python!","text":"<p>pip install or a python package manager can install ISPyPSA</p> <pre><code>pip install ispypsa\n</code></pre>"},{"location":"getting_started/#running-your-first-model","title":"Running your first model","text":"<ol> <li> <p>Create a new directory for storing your ispypsa model files, or if you followed    the <code>uv</code> install process just use the new-ispypsa-project directory you created.</p> </li> <li> <p>Download the default example config file and place in the new directory.</p> <p>\u2b07 Download ispypsa_config.yaml</p> </li> <li> <p>Edit the yaml config file so that the paths section matches your environment:</p> <ul> <li><code>parsed_traces_directory</code>: Base directory where trace data will be downloaded.      Choose a directory on a drive with at least 30 GB of free space.</li> <li><code>workbook_path</code>: Path where the ISP Excel workbook file will be downloaded (must end with <code>.xlsx</code>).      You need to give a complete file path and name, not just a path to a directory.</li> <li><code>parsed_workbook_cache</code>: Where extracted workbook data should be cached. This      could be anywhere but a subdirectory in your new directory is a good idea.</li> <li><code>run_directory</code>: Base directory where all model inputs and outputs will be      stored. This could be anywhere but a subdirectory in your new directory is a     good idea.</li> </ul> <p>The top of your yaml file should look something like this:</p> <pre><code> # ===== Path configuration =============================================================\n\n paths:\n   # The name of the ISPyPSA model run\n   # This name is used to select the output folder within `ispypsa_runs`\n   ispypsa_run_name: example_model_run\n\n   # Base directory where trace data will be downloaded\n   # The download task will create isp_2024 subdirectory automatically\n   parsed_traces_directory: \"data/trace_data\"\n\n   # Path where the ISP Excel workbook will be downloaded\n   workbook_path: \"data/2024-isp-inputs-and-assumptions-workbook.xlsx\"\n\n   # The path to the workbook table cache directory\n   parsed_workbook_cache: \"data/workbook_table_cache\"\n\n   # The run directory where all inputs and outputs will be stored\n   run_directory: \"ispypsa_runs\"\n</code></pre> </li> </ol> <p>Relative paths</p> <p>In this example the paths in the config should be relative to the directory where you will use the command line   to run the model. If you are using a new <code>uv</code> project then this will be the directory you just created, and   <code>data</code>, <code>data/trace_data</code>, <code>data/workbook_table_cache</code>, and <code>ispypsa_runs</code> should be new directories you have    created.</p> <ol> <li> <p>Using the command line inside the project/environment where <code>ispypsa</code> is installed download the ISP workbook:</p> uvplain python <pre><code>uv run ispypsa config=ispypsa_config.yaml download_workbook\n</code></pre> <pre><code>ispypsa config=ispypsa_config.yaml download_workbook\n</code></pre> <p>This will download the workbook to the path specified in your config file (<code>paths.workbook_path</code>). The workbook version is automatically determined from your config (<code>iasr_workbook_version</code>).</p> </li> <li> <p>Using the command line inside the project/environment where <code>ispypsa</code> is installed download the trace data:</p> uvplain python <pre><code>uv run ispypsa config=ispypsa_config.yaml download_trace_data\n</code></pre> <pre><code>ispypsa config=ispypsa_config.yaml download_trace_data\n</code></pre> <p>This will download trace data to your configured directory (<code>paths.parsed_traces_directory</code>). By default, the example dataset is downloaded, which is a smaller subset suitable for testing. The dataset type can be configured in your config file (<code>trace_data.dataset_type</code>) or overridden on the command line:</p> uvplain python <pre><code># Download the full dataset instead of example\nuv run ispypsa config=ispypsa_config.yaml trace_dataset_type=full download_trace_data\n</code></pre> <pre><code># Download the full dataset instead of example\nispypsa config=ispypsa_config.yaml trace_dataset_type=full download_trace_data\n</code></pre> </li> <li> <p>Using the command line inside the project/environment where <code>ispypsa</code> is installed    run the complete modeling workflow to test everything works correctly:</p> uvplain python <pre><code>uv run ispypsa config=ispypsa_config.yaml\n</code></pre> <pre><code>ispypsa config=ispypsa_config.yaml\n</code></pre> <p>Once the model run finishes you should have the following directory structure.</p> <pre><code>&lt;run_directory&gt;/\n\u2514\u2500\u2500 &lt;ispypsa_run_name&gt;/\n    \u251c\u2500\u2500 ispypsa_inputs/\n    \u2502   \u251c\u2500\u2500 build_costs.csv\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 pypsa_friendly/\n    \u2502   \u251c\u2500\u2500 buses.csv\n    \u2502   \u251c\u2500\u2500 ...\n    \u2502   \u251c\u2500\u2500 capacity_expansion_timeseries/\n    \u2502   \u2502   \u251c\u2500\u2500 demand_traces/\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 CNSW.parquet\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n    \u2502   \u2502   \u251c\u2500\u2500 solar_traces/\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 Bomen Solar Farm.parquet\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n    \u2502   \u2502   \u2514\u2500\u2500 wind_traces/\n    \u2502   \u2502       \u251c\u2500\u2500 Ararat Wind Farm.parquet\n    \u2502   \u2502       \u2514\u2500\u2500 ...\n    \u2502   \u2514\u2500\u2500 operational_timeseries/\n    \u2502       \u2514\u2500\u2500 (same structure as capacity_expansion_timeseries)\n    \u2514\u2500\u2500 outputs/\n        \u251c\u2500\u2500 capacity_expansion.nc\n        \u251c\u2500\u2500 capacity_expansion_results_viewer.html\n        \u251c\u2500\u2500 capacity_expansion_tables\n        \u251c\u2500\u2500 capacity_expansion_plots\n        \u251c\u2500\u2500 operational.nc\n        \u251c\u2500\u2500 operational_results_viewer.html\n        \u251c\u2500\u2500 operational_tables\n        \u2514\u2500\u2500 operational_plots\n</code></pre> </li> <li> <p>The previous model run used the default inputs for the ISP scenario specified in the    yaml file. To run model with different inputs you can edit the csv files in    <code>&lt;run_directory&gt;/&lt;ispypsa_run_name&gt;/ispypsa_inputs/</code>. To rerun the model    with the new inputs, use the same commands as before to rerun the    model. By default, <code>ispypsa</code> will detect that the inputs have changed and rerun only    modelling workflow steps which depend on the changed inputs, but not workflow steps    used to create the ispypsa input files.</p> uvplain python <pre><code>uv run ispypsa config=ispypsa_config.yaml\n</code></pre> <pre><code>ispypsa config=ispypsa_config.yaml\n</code></pre> </li> <li> <p>If you want to create the ispypsa inputs, so you can edit them, but not run the    complete model, use the following command:</p> uvplain python <pre><code>uv run ispypsa config=ispypsa_config.yaml create_ispypsa_inputs\n</code></pre> <pre><code>ispypsa config=ispypsa_config.yaml create_ispypsa_inputs\n</code></pre> </li> </ol> <p>Mastering the ispypsa command line tool (CLI)</p> <p>This example has shown you how to start using ISPyPSA through its command line   interface or CLI. To view the complete CLI documentation go to CLI</p> <p>The API alternative</p> <p>The alternative to using the CLI is to use the API, which offers more control than   the CLI but takes a little extra work.  To view the complete API documentation go   to API</p> <p>Understanding what's happening under the hood!</p> <p>When ISPyPSA is run from the command line the workflow executed is the same as in   example_workflow.py. If you want to understand   the workflow by tracing the API calls you can follow along in example_workflow.py   script. Alternatively, the Workflow section provides a   high level explanation.</p>"},{"location":"guide/","title":"Documentation guide","text":""},{"location":"guide/#getting-started","title":"Getting started","text":"<p>The Getting started section walks the user through:</p> <ul> <li>installation process</li> <li>obtaining the required data</li> <li>and running your first model</li> </ul>"},{"location":"guide/#workflow","title":"Workflow","text":"<p>The Workflow section explains the ISPyPSA data handling workflow, which converts AEMO inputs and data into a PyPSA model.</p>"},{"location":"guide/#modelling-method","title":"Modelling method","text":"<p>The Method section explains the conceptual details of the model ISPyPSA builds in PyPSA, and how this is controlled through user settings (config). By conceptual, we mean the descriptions are intended to describe the model in terms any modeller or analyst can understand, rather than explaining the Python code or the data processing used to achieve the intended method.</p> <p>Topics covered include:</p> <ul> <li>Transmission representation</li> <li>Generation representation</li> <li>Policy constraints</li> <li>Temporal resolution reduction</li> <li>Investment periodisation</li> <li>Custom constraints</li> </ul>"},{"location":"guide/#config","title":"Config","text":"<p>The Config explains the config file which can be used to control the modelling process.</p>"},{"location":"guide/#tables","title":"Tables","text":"<p>Coming soon</p> <p>Most of the inputs in ISPyPSA are specified through CSV files or Pandas DataFrames. The Tables section will provide detailed descriptions of each table, detailing the required columns, their units, and their effect on the model.</p> <ul> <li>ISPyPSA input tables describe the set of tables taken as inputs   to the translator, these tables are the recommended inputs   for most users to edit and create custom ISP scenarios.</li> <li>PyPSA friendly inputs describe the set of tables taken as inputs   to the model, these tables are provided as audit   points, but may also be useful for advanced users looking for fine tuned model control.</li> </ul>"},{"location":"guide/#api","title":"API","text":"<p>The API section explains the different ISPyPSA functions used to implement the modelling workflow. This documentation is helpful for users creating custom workflows, or looking to reuse individual elements of the ISPyPSA functionality within their own projects.</p>"},{"location":"guide/#examples","title":"Examples","text":"<p>The Examples section provides different examples of how ISPyPSA can be used. Currently, this just documents the default API and CLI workflow. In the future we will add a wider range of examples with various model configuration including both full scale examples, which show realistic NEM level models and simplified examples intended to help explain how ISPyPSA works.</p>"},{"location":"method/","title":"Modelling method","text":"<p>The method description provided here is intended as a plain english explanation that links the ISPyPSA inputs and model config to the formulation of the optimisation model in PyPSA.</p>"},{"location":"method/#overview","title":"Overview","text":"<p>Mixed-integer linear programing is used to represent the National Electricity Market's generation and transmission infrastructure, future costs, resource availability, demand for electricity, policy objectives, and operational and investment decisions. Optimisation can then be used to determine which combination of operational and investment decisions would result in the lowest total cost. The methods description which follows is a plain english description of how various aspects of the NEM are represented within the mixed-integer linear model.</p>"},{"location":"method/#nodal-representation","title":"Nodal representation","text":"<p>Each sub region and REZ is represented as a separate node within the model:</p> <ul> <li> <p>Generators, storages, and loads are located at particular nodes, with energy able to flow freely, without constraint or losses between any generator, storage, or load connected to the same node.</p> </li> <li> <p>The sub region nodes are defined by the <code>isp_sub_region_id</code> column in the ISPyPSA sub_regions input table.</p> </li> <li>The REZ nodes are defined by the <code>rez_id</code> column in the ISPyPSA renewable_energy_zones input table.</li> </ul>"},{"location":"method/#demand","title":"Demand","text":"<p>Demand is represented as a time varying quantity at each sub region node.</p> <ul> <li>Local supply at the node must equal demand, plus net transmission outflow, minus the   quantity of unserved energy at the node.</li> <li>The model config parameters unserved_energy.cost and unserved_energy.max_per_node can be used to configure cost and the allowable quantity of unserved energy. The default cost of unserved energy is set very high (10,000 $/MWh) to incentivise the optimisation to allow unserved energy only as a last resort. However, allowing some unserved energy helps prevent model infeasibility.</li> <li>The sub region demand data used is the demand data published by AEMO.</li> <li>The historical weather years, or reference years, used as a basis for deriving the time vary demand data are defined using the <code>reference_year_cycle</code> options in the config. More detail on reference years.</li> <li>The time varying quantity of demand at each node is also dependent on the model year. AEMO publishes demand data for every year in modelling horizon for each reference year, with demand changing over time due to economic growth, CER uptake, energy efficiency etc.</li> </ul>"},{"location":"method/#transmission","title":"Transmission","text":"<p>The transmission network is represented as the ability for energy to flow from node to node subject to a power constraint:</p> <ul> <li>Currently, transmission losses are not implemented in the model.</li> </ul>"},{"location":"method/#sub-region-to-sub-region","title":"Sub-region to sub-region","text":"<p>Transmission between sub regions is implemented with static constraints on power flow in each direction:</p> <ul> <li>The column <code>flow_path</code> in the ISPyPSA input table flow_paths defines the names of the model flow paths and the columns <code>node_from</code> and <code>node_to</code> define the nodes linked by the flow path.</li> <li>The two values <code>forward_direction_mw_summer_typical</code> and <code>reverse_direction_mw_summer_typical</code> from the ISPyPSA inputs table <code>flow_paths</code> set the limits on power flow in the forward and reverse flow directions.</li> <li>TODO: Implement time varying transmission limits making use of the peak demand and winter limits provided in the IASR workbook.</li> </ul>"},{"location":"method/#rez-exports","title":"REZ exports","text":"<p>REZ exports are implemented with a single static limit on power flow that applies in both directions of flow:</p> <ul> <li>The value <code>rez_transmission_network_limit_summer_typical</code> from the ISPyPSA input table <code>renewable_energy_zones</code> is used to set power flow limit.</li> <li>TODO: Implement time varying export limits making use of the peak demand and winter limits provided in the IASR workbook.</li> <li>If a NA or blank value is provided then the <code>rez_to_sub_region_transmission_default_limit</code> from the config file is used to set the limit. This is typically set to high value (1e5). Using the default limit is done for REZs where custom contraints are used to model REZ export limits, such that the static limit will not influence the optimisation.</li> </ul>"},{"location":"method/#custom-constraints","title":"Custom constraints","text":"<p>Custom constraints create arbitrary linear constraints linking the capacity or output of model components:</p> <ul> <li>In the base model implemented by the default work flow, custom constraints are used to represent complex network dynamics which link REZ exports to transmission flow between ISP sub regions and the dispatch of existing generators.</li> <li>The ISPyPSA tables custom_constraints_rhs and custom_constraint_lhs are used to define the custom linear constraints applied to the model. See the docs for these tables for further information on custom constraint implementation.</li> </ul>"},{"location":"method/#generation","title":"Generation","text":"<p>Generation is represented as a time-varying quantity for each generator:</p> <ul> <li>Variable renewable energy (VRE) generation data traces are the generation data published by AEMO for each project or REZ and resource type. These traces set the upper limit on VRE generator output in each modelled snapshot.</li> <li>The historical weather years, or reference years, used as a basis for deriving the time varying generation data are defined using the <code>reference_year_cycle</code> options in the config. More detail on reference years.</li> <li>The time varying quantity of generation at each node is also dependent on the model year. AEMO publishes generation data for every year in modelling horizon for each reference year.</li> <li>Other non-VRE generation is currently modelled under static output limits set by each generator's maximum capacity <code>maximum_capacity_mw</code> and minimum stable generation <code>minimum_load_mw</code> or <code>minimum_stable_level_%</code> (where defined).</li> <li>Generator dispatch is optimised at each snapshot to meet demand at the lowest cost while meeting the output constraints described above.</li> </ul>"},{"location":"method/#storage","title":"Storage","text":"<p>Storage charging and discharging behaviour is also represented as a time-varying quantity for each storage unit in the model:</p> <ul> <li>Charging and discharging efficiencies are defined for each battery and applied to charge/discharge in each snapshot accordingly.</li> <li>The state of charge of each battery in each snapshot is determined by the previous state of charge plus energy charged minus energy discharged (with efficiencies applied).</li> <li>Charge and discharge power and energy in each snapshot are limited by the <code>maximum_capacity_mw</code> and <code>maximum_capacity_mw</code> \\(\\times\\) <code>storage_duration_hours</code> properties of each battery, as well as the available state of charge.</li> <li>Battery charging and discharging behaviour is optimised for each snapshot to meet demand at lowest cost, while subject to energy balance constraints.</li> </ul>"},{"location":"method/#reference-years","title":"Reference years","text":"<p>Weather reference years are used ensure weather correlations are consistent between demand and renewable energy availability, and to ensure the modelling considers diverse weather conditions:</p> <ul> <li>For each model year between the model start year and model end year, a historical reference year is chosen from which to derive both demand and renewable energy availability.</li> <li>In the model config the inputs temporal.capacity_expansion.reference_year_cycle and temporal.operational.reference_year_cycle are used to specify the ordering of reference years.</li> <li>If the <code>reference_year_cycle</code> is shorter than the model horizon then the cycle is repeated as many times as needed.</li> </ul>"},{"location":"method/#temporal-aggregation","title":"Temporal aggregation","text":"<p>Temporal aggregation is used condense the temporal representation of time varying quantities within the model to improve computational tractability while retaining good representation of their characteristics:</p> <ul> <li>If no temporal aggregation is specified then a full half hourly representation is retained for each year.</li> <li>If temporal aggregation is used, the contribution of each time interval in the model are scaled up so that operational costs, fuel use, and emissions are equivalent to a fully time resolved model. See Investment periodisation and discounting for further detail on interval weighting.</li> </ul>"},{"location":"method/#representative-weeks","title":"Representative weeks","text":"<p>The <code>representative_weeks</code> input in the model config can be used to specify particular weeks to include in the temporal representation:</p> <ul> <li>A list of integers is provided for <code>representative_weeks</code> and these number weeks with in the year will retained in the time sequence used by the model. For example, if <code>[1, 25]</code> are provided then the first and twenty fifth weeks in each model year will be retained.</li> <li>Counting of weeks within the year starts from the first whole week (Monday to Sunday) which falls within the year.</li> <li>Separate sets of representative weeks can be specified for capacity expansion and operational modelling using the temporal.capacity_expansion.aggregation.representative_weeks and temporal.operational.aggregation.representative_weeks config inputs.</li> <li>If both <code>representative_weeks</code> and <code>named_representative_weeks</code> are given, then weeks from both aggregations are used.   If the same week is selected twice only one instance is kept.</li> <li>TODO: Clarify that representative weeks treated sequential by PyPSA.</li> </ul>"},{"location":"method/#named-representative-weeks","title":"Named representative weeks","text":"<p>The <code>named_representative_weeks</code> input in the model config can be used to specify particular weeks to include in the temporal representation:</p> <ul> <li>A list of strings is provided for <code>named_representative_weeks</code> specifying the weeks to be extracted from the yearly data. For example, [\"residual-peak-demand\", \"residual-minimum-demand\"].</li> <li> <p>The named representative week options are:</p> <ul> <li>peak-demand: Week with highest instantaneous demand</li> <li>minimum-demand: Week with lowest instantaneous demand</li> <li>peak-consumption: Week with highest average demand (energy consumption)</li> <li>residual-peak-demand: Week with highest demand net of renewables</li> <li>residual-minimum-demand: Week with lowest demand net of renewables</li> <li>residual-peak-consumption: Week with highest average demand net of renewables</li> </ul> </li> <li> <p>Only whole weeks which fall entirely within a model year are considered for selection.</p> </li> <li>Separate sets of named representative weeks can be specified for capacity expansion and operational modelling using the temporal.capacity_expansion.aggregation.named_representative_weeks and temporal.operational.aggregation.named_representative_weeks config inputs.</li> <li>If both <code>representative_weeks</code> and <code>named_representative_weeks</code> are given, then weeks from both aggregations are used.   If the same week is selected twice only one instance is kept.</li> </ul>"},{"location":"method/#capacity-expansion","title":"Capacity expansion","text":"<p>Capacity expansion is the first modelling phase. In this phase investment in generation and capacity expansion are co-optimised with operational dispatch decisions. The optimisation assumes perfect foresight.</p>"},{"location":"method/#investment-periodisation-and-discounting","title":"Investment periodisation and discounting","text":"<p>Capacity expansion decisions are periodised with investment decisions made at the start of each multiyear period:</p> <ul> <li>Instead of investment decisions being available every year, they are only available at the being of each investment period. This reduces computational complexity.</li> <li>The config input temporal.capacity_expansion.investment_periods can be used to specify the investment periodisation. For example, specify <code>[2025, 2030]</code> will create two investment periods in the model, one starting at the beginning of 2025 and ending at the beginning of 2030, and a second period starting at the beginning of 2030 and lasting to the end of the modelling horizon.</li> <li>The periodisation is not myopic. The optimisation considers investment across all periods simultaneously.</li> </ul> <p>The periodisation is also used to structure the application of discount rates to the model:</p> <ul> <li>An objective function weighting is calculated for each investment period and applied to both capacity expansion and operational costs accrued during an investment period.</li> <li>The weighting for a period is calculated as the sum of discount factors used to adjust costs to Net Present Value on yearly basis over the duration of an investment period:</li> </ul> \\[ w_{p}=\\sum_{i=t_{period\\_start}}^{t_{period\\_end}}\\frac{1}{(1 + r)^i} \\] <p>Where \\(r\\) is the discount_rate specified in the model config.</p> <ul> <li>The sum of discount factors rather than the average is used such that capital cost which are applied once per investment period are scaled up by the number of years in the investment period. This is equivalent to taking the average discount factor for the period and multiplying by the number of years.</li> <li>Note: because the period weighting is also applied to operational costs, the interval level weighting/scaling used to account for temporal aggregation is calculated so that all intervals in an investment period have the equivalent weight of one year of full time resolved intervals. Then, when the period weighting is applied the costs are scaled back up to match the number of years in the investment period.</li> </ul>"},{"location":"method/#transmission_1","title":"Transmission","text":"<p>The model considers the expansion of sub region to sub region and REZ export transmission capacity at the start of every investment period:</p> <ul> <li>The capital costs in $/MW for transmission expansion are provided in the ISPyPSA tables flow_path_expansion_costs and rez_transmission_expansion_costs. The costs used are those in cost columns corresponding to the start years of each investment period.</li> <li>Costs are annuitised according to the following formula:</li> </ul> \\[ c_{a} =\\frac{c_{o} \\times r }{1 - (1 + r)^{-t}} \\] <p>Where \\(c_{a}\\) is the annuitised cost, \\(c_{o}\\) is the overnight build cost, \\(r\\) is the WACC, and \\(t\\) is the network cost annuitisation_lifetime.</p> <ul> <li>If a transmission flow path or REZ connection is expanded in one investment period it's annuitised cost multiplied by the size of the expansion is incurred in all subsequent investment periods. Transmission expansions are not considered to retire.</li> <li>The value in column <code>additional_network_capacity_mw</code> in flow_path_expansion_costs and rez_transmission_expansion_costs sets a limit on the total expansion allowed for each flow path and REZ connection.</li> <li>Where REZ export limits are set by custom constraints the expansion of the REZ connection is modelled as relaxing the custom constraint limit.</li> </ul>"},{"location":"method/#generation_1","title":"Generation","text":"<p>Generator capacities are decided by the model at the start of each investment period and for each node:</p> <ul> <li>The model currently considers all ECAA generators that are active (not retired) during the model investment periods. These projects have set capacities and are not extendable during the capacity expansion modelling, and have fixed retirement dates.</li> <li>New entrant generator are extendable in the model, and the optimisation determines the capacity of new generation to be built at each node in each investment period.</li> <li>Capital costs in $/MW for new entrant generators are annuitised according to the following formula:</li> </ul> <p>$$   c_{a} =\\frac{c_{o} \\times r }{1 - (1 + r)^{-t}}   $$</p> <p>Where \\(c_{a}\\) is the annuitised cost and, \\(r\\) is the WACC, and \\(t\\) is the generator lifetime.   \\(c_{o}\\) includes the overnight build cost (adjusted by locational cost factors), any applicable connection costs   and/or additional system strength connection costs, and fixed operational costs. - Marginal costs in $/MWh are calculated for all generators for each model snapshot based on dynamic fuel prices, generator heat rates and variable operational costs defined in the input tables. Alternatively a static value can be set for a subset or all generators to simplify the model. - Where build or resource limit constraints are defined for VRE generation in specific REZs, these are set by custom constraints. Some resource limits can be relaxed up to the corresponding build limit for the specified resource type and REZ.</p>"},{"location":"method/#operational","title":"Operational","text":"<p>Operational is the second modelling phase. In this modelling phase capacity expansion decisions are taken as fixed, at the values found during the capacity expansion phase, and only operational decisions are optimised:</p> <ul> <li>Operational optimisation is done using a rolling horizon, breaking up the optimisation into many smaller problems which can be solved sequentially.</li> <li>The rolling horizon formulation has two additional config parameters horizon and overlap. <code>horizon</code> controls the number of intervals in each sub problem and <code>overlap</code> controls the number of intervals overlapping between each sub problem. If <code>overlap</code> is greater than zero then the second optimisations decisions are recorded as the model dispatch values. The overlap gives each sub problem a look ahead so it is not completely naive to future conditions.</li> <li>Fixing investment decisions and using rolling horizons reduce model complexity and speed up run time, allowing for operation to be modelled with greater temporal resolution than capacity expansion.</li> </ul>"},{"location":"workflow/","title":"Workflow","text":"<p>The ISPyPSA workflow consists of a series of data handling and manipulation steps to convert input data from the AEMO provided format to a format consistent with the PyPSA API. Once the PyPSA friendly inputs are created then a PyPSA network object is created and optimised to run the capacity expansion and operational modelling. The steps of the workflow are outlined below.</p>"},{"location":"workflow/#input-data-downloading","title":"Input data downloading","text":"<p>ISPyPSA requires two main types of input data: the IASR workbook containing model parameters and assumptions, and trace data containing wind, solar, and demand time series. Both can be downloaded using the ISPyPSA CLI or API.</p>"},{"location":"workflow/#workbook","title":"Workbook","text":"<p>The IASR workbook is downloaded from OpenISP's public archive. The workbook download can be run using either the ISPyPSA CLI or API:</p> CLIAPI <pre><code>uv run ispypsa config=ispypsa_config.yaml download_workbook\n</code></pre> <pre><code>from ispypsa.data_fetch import fetch_workbook\n\nfetch_workbook(\n    workbook_version=\"6.0\",\n    save_path=\"path/to/save/iasr_workbook.xlsx\"\n)\n</code></pre>"},{"location":"workflow/#trace-data","title":"Trace data","text":"<p>The trace data contains time series for wind and solar resource availability, and demand. This data has been preprocessed using isp-trace-parser and is hosted on OpenISP's public archive. The dataset type (full or example) and year are specified in the configuration file. The trace data download can be run using either the ISPyPSA CLI or API:</p> CLIAPI <pre><code>uv run ispypsa config=ispypsa_config.yaml download_trace_data\n</code></pre> <pre><code>from ispypsa.data_fetch import fetch_trace_data\n\nfetch_trace_data(\n    dataset_type=\"example\",\n    dataset_year=2024,\n    save_directory=\"path/to/save/directory\"\n)\n</code></pre>"},{"location":"workflow/#workbook-parsing","title":"Workbook parsing","text":"<p>AEMO provides many of the inputs for its ISP models in the Inputs Assumptions Scenarios Report (IASR) MS Excel workbook. The workbook parsing step of the workflow extracts this data from the workbook and saves the data in a set of CSV files, with each CSV file corresponding to a table from the workbook. The parsing is handled by the external package isp-workbook-parser, but running the parser is integrated directly into the ISPyPSA workflow via a call to the isp-workbook-parser API. The workbook parsing can be run using either the ISPyPSA CLI or API:</p> CLIAPI <pre><code>uv run ispypsa config=ispypsa_config.yaml cache_required_iasr_workbook_tables\n</code></pre> <pre><code>from ispypsa.iasr_table_caching import build_local_cache\n\nbuild_local_cache(\n    cache_path=\"path/to/cache/location\",\n    workbook_path=\"path/to/iasr_workbook.xlsx\",\n    iasr_workbook_version=\"6.0\"\n)\n</code></pre>"},{"location":"workflow/#wind-solar-and-demand-trace-data-parsing","title":"Wind, solar, and demand trace data parsing","text":"<p>AEMO provides time series data for wind and solar resource availability, and demand as CSVs. The trace parsing performs data format and naming conventions adjustments to integrate the data smoothly with the rest of the ISPyPSA workflow. The parsing is handled by the external package isp-trace-parser, the trace parsing is not integrated directly into the ISPyPSA workflow due to the long computational time required, instead pre-parsed trace data is provided during the initial input downloading workflow step.</p>"},{"location":"workflow/#templating","title":"Templating","text":"<p>The templating step of the workflow extracts the input data required to run a particular ISP scenario from the parsed workbook tables. The data is also reformatted to create a more concise table set, while attempting to largely maintain the core data structures established by AEMO in the IASR workbook. This step is called templating because it produces a template set of inputs based on an ISP scenario which can then be used to run an ISPyPSA model. The tables produced by the templating are referred to as ISPyPSA input tables. The templating step can be run using either ISPyPSA CLI or API.</p> CLIAPI <pre><code>uv run ispypsa config=config.yaml create_ispypsa_inputs\n</code></pre> <pre><code>from pathlib import Path\n\nfrom ispypsa.config import load_config\nfrom ispypsa.data_fetch import read_csvs, write_csvs\nfrom ispypsa.templater import (\n    create_ispypsa_inputs_template,\n    load_manually_extracted_tables,\n)\n\n# Load model config\nconfig_path = Path(\"ispypsa_config.yaml\")\nconfig = load_config(config_path)\n\n# Setup directory paths\nparsed_workbook_cache = Path(config.paths.parsed_workbook_cache)\nrun_directory = Path(config.paths.run_directory)\nispypsa_input_tables_directory = (\n    run_directory / config.paths.ispypsa_run_name / \"ispypsa_inputs\"\n)\n\n# Get raw IASR tables\niasr_tables = read_csvs(parsed_workbook_cache)\nmanually_extracted_tables = load_manually_extracted_tables(\n    config.iasr_workbook_version\n)\n\n# Create ISPyPSA inputs from IASR tables.\nispypsa_tables = create_ispypsa_inputs_template(\n    config.scenario,\n    config.network.nodes.regional_granularity,\n    iasr_tables,\n    manually_extracted_tables,\n    config.filter_by_nem_regions,\n    config.filter_by_isp_sub_regions,\n)\nwrite_csvs(ispypsa_tables, ispypsa_input_tables_directory)\n</code></pre>"},{"location":"workflow/#translating","title":"Translating","text":"<p>The translating step of the workflow converts ISPyPSA inputs tables and parsed trace data into a format consistent with the structure of the PyPSA API, these are referred to as the PyPSA friendly inputs. This is referred to as the translating step because we are translating the inputs from ISPyPSA format to PyPSA friendly format. The translating step can be run using either the ISPyPSA CLI or API.</p> CLIAPI <pre><code>uv run ispypsa config=config.yaml create_pypsa_friendly_inputs\n</code></pre> <pre><code>from pathlib import Path\n\nfrom ispypsa.config import load_config\nfrom ispypsa.data_fetch import read_csvs, write_csvs\nfrom ispypsa.translator import (\n    create_pypsa_friendly_inputs,\n    create_pypsa_friendly_timeseries_inputs,\n)\n\n# Load model config\nconfig_path = Path(\"ispypsa_config.yaml\")\nconfig = load_config(config_path)\n\n# Setup directory paths\nparsed_traces_directory = (\n    Path(config.paths.parsed_traces_directory) / f\"isp_{config.trace_data.dataset_year}\"\n)\nrun_directory = Path(config.paths.run_directory)\nispypsa_input_tables_directory = (\n    run_directory / config.paths.ispypsa_run_name / \"ispypsa_inputs\"\n)\npypsa_friendly_inputs_location = (\n    run_directory / config.paths.ispypsa_run_name / \"pypsa_friendly\"\n)\ncapacity_expansion_timeseries_location = (\n    pypsa_friendly_inputs_location / \"capacity_expansion_timeseries\"\n)\n\n# Load ISPyPSA input tables\nispypsa_tables = read_csvs(ispypsa_input_tables_directory)\n\n# Translate ISPyPSA format to a PyPSA friendly format.\npypsa_friendly_input_tables = create_pypsa_friendly_inputs(config, ispypsa_tables)\n\n# Create timeseries inputs\npypsa_friendly_input_tables[\"snapshots\"] = create_pypsa_friendly_timeseries_inputs(\n    config,\n    \"capacity_expansion\",\n    ispypsa_tables,\n    pypsa_friendly_input_tables[\"generators\"],\n    parsed_traces_directory,\n    capacity_expansion_timeseries_location,\n)\n\nwrite_csvs(pypsa_friendly_input_tables, pypsa_friendly_inputs_location)\n</code></pre>"},{"location":"workflow/#create-operational-timeseries-data","title":"Create operational timeseries data","text":"<p>Optionally, if you want to run the operational model after running capacity expansion you can also create the PyPSA friendly operational timeseries data at this stage.</p> CLIAPI <pre><code>uv run ispypsa config=config.yaml create_operational_timeseries\n</code></pre> <pre><code>from pathlib import Path\n\nfrom ispypsa.config import load_config\nfrom ispypsa.data_fetch import read_csvs, write_csvs\nfrom ispypsa.translator import create_pypsa_friendly_timeseries_inputs\n\n# Load model config\nconfig_path = Path(\"ispypsa_config.yaml\")\nconfig = load_config(config_path)\n\n# Setup directory paths\nparsed_traces_directory = (\n    Path(config.paths.parsed_traces_directory) / f\"isp_{config.trace_data.dataset_year}\"\n)\nrun_directory = Path(config.paths.run_directory)\nispypsa_input_tables_directory = (\n    run_directory / config.paths.ispypsa_run_name / \"ispypsa_inputs\"\n)\npypsa_friendly_inputs_location = (\n    run_directory / config.paths.ispypsa_run_name / \"pypsa_friendly\"\n)\noperational_timeseries_location = (\n    pypsa_friendly_inputs_location / \"operational_timeseries\"\n)\n\n# Load ISPyPSA input tables\nispypsa_tables = read_csvs(ispypsa_input_tables_directory)\n\n# Load PyPSA friendly input tables (for generators)\npypsa_friendly_input_tables = read_csvs(pypsa_friendly_inputs_location)\n\n# Create timeseries inputs\noperational_snapshots = create_pypsa_friendly_timeseries_inputs(\n    config,\n    \"operational\",\n    ispypsa_tables,\n    pypsa_friendly_input_tables[\"generators\"],\n    parsed_traces_directory,\n    operational_timeseries_location,\n)\n\nwrite_csvs({\"operational_snapshots\": operational_snapshots}, pypsa_friendly_inputs_location)\n</code></pre>"},{"location":"workflow/#pypsa-building-and-run","title":"PyPSA building and run","text":""},{"location":"workflow/#capacity-expansion-model","title":"Capacity expansion model","text":"<p>Once the PyPSA friendly inputs have been created a PyPSA network object can be created, inputs loaded into the network object, and the capacity expansion optimisation run. The PyPSA object needs to be built and run within a single workflow step as custom constraints are not preserved when the PyPSA network object is saved to disk.</p> CLIAPI <pre><code>uv run ispypsa config=config.yaml create_and_run_capacity_expansion_model\n</code></pre> <pre><code>from pathlib import Path\n\nfrom ispypsa.config import load_config\nfrom ispypsa.data_fetch import read_csvs\nfrom ispypsa.pypas_build import build_pypsa_network, save_pypsa_network\n\n# Load model config\nconfig_path = Path(\"ispypsa_config.yaml\")\nconfig = load_config(config_path)\n\n# Setup directory paths\nrun_directory = Path(config.paths.run_directory)\npypsa_friendly_inputs_location = (\n    run_directory / config.paths.ispypsa_run_name / \"pypsa_friendly\"\n)\ntimeseries_location = (\n    pypsa_friendly_inputs_location / \"capacity_expansion_timeseries\"\n)\npypsa_outputs_directory = (\n    run_directory / config.paths.ispypsa_run_name / \"outputs\"\n)\n\n# Load pypsa friendly inputs\npypsa_tables = read_csvs(pypsa_friendly_inputs_location)\n\n# Build a PyPSA network object.\nnetwork = build_pypsa_network(\n    pypsa_tables,\n    timeseries_location,\n)\n\n# Solve for least cost operation/expansion\n# Never use network.optimize() as this will remove custom constraints.\nnetwork.optimize.solve_model(solver_name=config.solver)\n\n# Save results.\nsave_pypsa_network(\n    network,\n    pypsa_outputs_directory,\n    \"capacity_expansion\"\n)\n</code></pre>"},{"location":"workflow/#operational-model","title":"Operational model","text":"<p>After the capacity expansion model has run, an operational model, usually with a higher temporal resolution, can be built and run. The operational model build takes the PyPSA network object used for capacity expansion and fixes generator, storage, and transmission line capacities at their optimal values. Then the timeseries load and resource availability timeseries data is updated to match the operational model temporal resolution, custom constraints are rebuilt, and the optimisation rerun. Note, the operational optimisation uses rolling horizon to allow for greater temporal resolution, the fixing of the unit and transmission capacities also help limit computational complexity.</p> CLIAPI <pre><code>uv run ispypsa config=config.yaml create_and_run_operational_model\n</code></pre> <pre><code>from pathlib import Path\n\nimport pypsa\n\nfrom ispypsa.config import load_config\nfrom ispypsa.data_fetch import read_csvs\nfrom ispypsa.pypas_build import update_network_timeseries, save_pypsa_network\n\n# Load model config\nconfig_path = Path(\"ispypsa_config.yaml\")\nconfig = load_config(config_path)\n\n# Setup directory paths\nrun_directory = Path(config.paths.run_directory)\npypsa_friendly_inputs_location = (\n    run_directory / config.paths.ispypsa_run_name / \"pypsa_friendly\"\n)\noperational_timeseries_location = (\n    pypsa_friendly_inputs_location / \"operational_timeseries\"\n)\npypsa_outputs_directory = (\n    run_directory / config.paths.ispypsa_run_name / \"outputs\"\n)\ncapacity_expansion_pypsa_file = (\n    pypsa_outputs_directory / \"capacity_expansion.nc\"\n)\n\n# Load pypsa friendly inputs\npypsa_tables = read_csvs(pypsa_friendly_inputs_location)\n\n# Load the capacity expansion network\nnetwork = pypsa.Network(capacity_expansion_pypsa_file)\n\n# Update network timeseries\nupdate_network_timeseries(\n    network,\n    pypsa_tables,\n    pypsa_tables[\"operational_snapshots\"],\n    operational_timeseries_location,\n)\n\nnetwork.optimize.fix_optimal_capacities()\n\n# Never use network.optimize() as this will remove custom constraints.\nnetwork.optimize.optimize_with_rolling_horizon(\n    horizon=config.temporal.operational.horizon,\n    overlap=config.temporal.operational.overlap,\n)\n\nsave_pypsa_network(\n    network,\n    pypsa_outputs_directory,\n    \"operational\"\n)\n</code></pre>"},{"location":"examples/example_api_workflow/","title":"Default workflow","text":"<p>Below is an example of the default ISPyPSA workflow implemented using the Python API. This is the same workflow which the CLI follows. To use this workflow you simply need to edit the code to point at an ispypsa_config.yaml file of your choice, then run the Python script.</p> <pre><code>from pathlib import Path\n\nfrom ispypsa.config import load_config\nfrom ispypsa.data_fetch import read_csvs, write_csvs\nfrom ispypsa.iasr_table_caching import build_local_cache\nfrom ispypsa.logging import configure_logging\nfrom ispypsa.plotting import (\n    create_plot_suite,\n    generate_results_website,\n    save_plots,\n)\nfrom ispypsa.pypsa_build import (\n    build_pypsa_network,\n    save_pypsa_network,\n    update_network_timeseries,\n)\nfrom ispypsa.results import (\n    extract_regions_and_zones_mapping,\n    extract_tabular_results,\n)\nfrom ispypsa.templater import (\n    create_ispypsa_inputs_template,\n    load_manually_extracted_tables,\n)\nfrom ispypsa.translator import (\n    create_pypsa_friendly_inputs,\n    create_pypsa_friendly_timeseries_inputs,\n)\n\n# Load model config.\nconfig_path = Path(\"ispypsa_config.yaml\")\nconfig = load_config(config_path)\n\n# Load base paths from config\nparsed_workbook_cache = Path(config.paths.parsed_workbook_cache)\nparsed_traces_directory = (\n    Path(config.paths.parsed_traces_directory) / f\"isp_{config.trace_data.dataset_year}\"\n)\nworkbook_path = Path(config.paths.workbook_path)\nrun_directory = Path(config.paths.run_directory)\n\n# Construct full paths from base paths\nispypsa_input_tables_directory = (\n    run_directory / config.paths.ispypsa_run_name / \"ispypsa_inputs\"\n)\npypsa_friendly_inputs_location = (\n    run_directory / config.paths.ispypsa_run_name / \"pypsa_friendly\"\n)\ncapacity_expansion_timeseries_location = (\n    pypsa_friendly_inputs_location / \"capacity_expansion_timeseries\"\n)\noperational_timeseries_location = (\n    pypsa_friendly_inputs_location / \"operational_timeseries\"\n)\npypsa_outputs_directory = run_directory / config.paths.ispypsa_run_name / \"outputs\"\ncapacity_expansion_tabular_results_directory = (\n    pypsa_outputs_directory / \"capacity_expansion_tables\"\n)\ncapacity_expansion_plot_results_directory = (\n    pypsa_outputs_directory / \"capacity_expansion_plots\"\n)\noperational_tabular_results_directory = pypsa_outputs_directory / \"operational_tables\"\noperational_plot_results_directory = pypsa_outputs_directory / \"operational_plots\"\n\n# Create output directories if they don't exist\nparsed_workbook_cache.mkdir(parents=True, exist_ok=True)\nispypsa_input_tables_directory.mkdir(parents=True, exist_ok=True)\npypsa_friendly_inputs_location.mkdir(parents=True, exist_ok=True)\ncapacity_expansion_timeseries_location.mkdir(parents=True, exist_ok=True)\noperational_timeseries_location.mkdir(parents=True, exist_ok=True)\npypsa_outputs_directory.mkdir(parents=True, exist_ok=True)\ncapacity_expansion_tabular_results_directory.mkdir(parents=True, exist_ok=True)\ncapacity_expansion_plot_results_directory.mkdir(parents=True, exist_ok=True)\noperational_tabular_results_directory.mkdir(parents=True, exist_ok=True)\noperational_plot_results_directory.mkdir(parents=True, exist_ok=True)\n\nconfigure_logging()\n\n# Build the local cache from the workbook\nbuild_local_cache(parsed_workbook_cache, workbook_path, config.iasr_workbook_version)\n\n# Load ISP IASR data tables.\niasr_tables = read_csvs(parsed_workbook_cache)\nmanually_extracted_tables = load_manually_extracted_tables(config.iasr_workbook_version)\n\n# Create ISPyPSA inputs from IASR tables.\nispypsa_tables = create_ispypsa_inputs_template(\n    config.scenario,\n    config.network.nodes.regional_granularity,\n    iasr_tables,\n    manually_extracted_tables,\n    config.filter_by_nem_regions,\n    config.filter_by_isp_sub_regions,\n)\nwrite_csvs(ispypsa_tables, ispypsa_input_tables_directory)\n\n# Suggested stage of user interaction:\n# At this stage of the workflow the user can modify ispypsa input files, either\n# manually or programmatically, to run alternative scenarios using the template\n# generated from the chosen ISP scenario.\n\n# Translate ISPyPSA format to a PyPSA friendly format.\npypsa_friendly_input_tables = create_pypsa_friendly_inputs(config, ispypsa_tables)\n\n# Create timeseries inputs and snapshots\npypsa_friendly_input_tables[\"snapshots\"] = create_pypsa_friendly_timeseries_inputs(\n    config,\n    \"capacity_expansion\",\n    ispypsa_tables,\n    pypsa_friendly_input_tables[\"generators\"],\n    parsed_traces_directory,\n    capacity_expansion_timeseries_location,\n)\n\nwrite_csvs(pypsa_friendly_input_tables, pypsa_friendly_inputs_location)\n\n# Build a PyPSA network object.\nnetwork = build_pypsa_network(\n    pypsa_friendly_input_tables,\n    capacity_expansion_timeseries_location,\n)\n\n# Solve for least cost operation/expansion\n# Never use network.optimize() as this will remove custom constraints.\nnetwork.optimize.solve_model(solver_name=config.solver)\n\n# Save capacity expansion results\nsave_pypsa_network(network, pypsa_outputs_directory, \"capacity_expansion\")\ncapacity_expansion_results = extract_tabular_results(network, ispypsa_tables)\ncapacity_expansion_results[\"regions_and_zones_mapping\"] = (\n    extract_regions_and_zones_mapping(ispypsa_tables)\n)\nwrite_csvs(capacity_expansion_results, capacity_expansion_tabular_results_directory)\n\n# Create and save capacity expansion plots\ncapacity_expansion_plots = create_plot_suite(capacity_expansion_results)\nsave_plots(capacity_expansion_plots, capacity_expansion_plot_results_directory)\n\n# Generate capacity expansion results website\ngenerate_results_website(\n    capacity_expansion_plots,\n    capacity_expansion_plot_results_directory,\n    pypsa_outputs_directory,\n    site_name=f\"{config.paths.ispypsa_run_name}\",\n    output_filename=\"capacity_expansion_results_viewer.html\",\n    subtitle=\"Capacity Expansion Analysis\",\n    regions_and_zones_mapping=capacity_expansion_results[\"regions_and_zones_mapping\"],\n)\n\n# Operational modelling extension\noperational_snapshots = create_pypsa_friendly_timeseries_inputs(\n    config,\n    \"operational\",\n    ispypsa_tables,\n    pypsa_friendly_input_tables[\"generators\"],\n    parsed_traces_directory,\n    operational_timeseries_location,\n)\n\nwrite_csvs(\n    {\"operational_snapshots\": operational_snapshots}, pypsa_friendly_inputs_location\n)\n\nupdate_network_timeseries(\n    network,\n    pypsa_friendly_input_tables,\n    operational_snapshots,\n    operational_timeseries_location,\n)\n\nnetwork.optimize.fix_optimal_capacities()\n\n# Never use network.optimize() as this will remove custom constraints.\nnetwork.optimize.optimize_with_rolling_horizon(\n    horizon=config.temporal.operational.horizon,\n    overlap=config.temporal.operational.overlap,\n)\n\nsave_pypsa_network(network, pypsa_outputs_directory, \"operational\")\n\n# Extract and save operational results\noperational_results = extract_tabular_results(network, ispypsa_tables)\noperational_results[\"regions_and_zones_mapping\"] = extract_regions_and_zones_mapping(\n    ispypsa_tables\n)\nwrite_csvs(operational_results, operational_tabular_results_directory)\n\n# Create and save operational plots\noperational_plots = create_plot_suite(operational_results)\nsave_plots(operational_plots, operational_plot_results_directory)\n\n# Generate operational results website\ngenerate_results_website(\n    operational_plots,\n    operational_plot_results_directory,\n    pypsa_outputs_directory,\n    site_name=f\"{config.paths.ispypsa_run_name}\",\n    output_filename=\"operational_results_viewer.html\",\n    subtitle=\"Operational Analysis\",\n    regions_and_zones_mapping=operational_results[\"regions_and_zones_mapping\"],\n)\n</code></pre>"},{"location":"examples/example_config/","title":"Default config file","text":"<p>Below is an example ispypsa_config.yaml file, the file configures a simple fast running model designed to be useful for learning and testing purposes. The model implemented by the config:</p> <ul> <li>Optimises capacity expansion to met demand in 2050 in a single build step</li> <li>Only considers the NSW region</li> <li>Is based on 2024 ISP Step Change scenario assumption</li> <li>Retains existing capacity which has not reached retirement age by 2050</li> <li>Uses three weeks of timeseries data based on 2018 reference year data the weeks in 2050 with residual peak demand, peak-consumption, and residual minimum demand are used.</li> </ul> <pre><code># ===== Paths ==========================================================================\n\npaths:\n  # The path to the folder containing parsed demand, wind and solar traces.\n  # If set to ENV the path will be retrieved from the environment variable\n  # \"PATH_TO_PARSED_TRACES\"\n  parsed_traces_directory: ENV\n\n  # The path to the ISP workbook Excel file\n  workbook_path: \"../isp-workbook-parser/workbooks/6.0/2024-isp-inputs-and-assumptions-workbook.xlsx\"\n\n  # The path to the workbook table cache directory\n  parsed_workbook_cache: \"ispypsa_runs/workbook_table_cache\"\n\n  # The run directory where all inputs and outputs will be stored\n  run_directory: \"ispypsa_runs\"\n\n  # The name of the ISPyPSA model run\n  # This name is used to select the output folder within `ispypsa_runs`\n  ispypsa_run_name: overnight_build_nsw_2050\n\n\n# ===== Trace Data =====================================================================\n\ntrace_data:\n  # The type of trace dataset to download when using CLI download tasks\n  # Options: \"example\" (smaller, for testing) or \"full\" (complete dataset)\n  # Default: \"example\"\n  dataset_type: example\n\n  # The year of trace dataset to download when using CLI download tasks\n  # Currently only 2024 is supported\n  # Default: 2024\n  dataset_year: 2024\n\n\n# ===== ISPyPSA templating =============================================================\n\n# The version of IASR workbook that the template inputs are generated from.\niasr_workbook_version: \"6.0\"\n\n# The ISP scenario for which to generate ISPyPSA inputs\nscenario: Step Change\n\n\n# ===== Financial/Economic =============================================================\n\n# Weighted average cost of capital for annuitisation of generation and transmission\n# costs, as a fraction, i.e. 0.07 is 7%.\nwacc: 0.07\n# Discount rate applied to model objective function, as a fraction, i.e. 0.07 is 7%.\ndiscount_rate: 0.05\n\n\n# ===== Unserved energy ================================================================\n\nunserved_energy:\n  # Cost of unserved energy in $/MWh. Set to 'None' to disable unserved energy generators.\n  cost: 10000.0\n  # Maximum capacity of each unserved energy generator in MW. Defaults to 1e5 (100,000 MW).\n  max_per_node: 100000.0\n\n\n# ===== Regional scope =================================================================\n\n# Filter the model by NEM regions or ISP sub regions.\nfilter_by_nem_regions: [NSW]\n# filter_by_isp_sub_regions: [NNSW, CNSW]\n\n\n# ===== Network ========================================================================\n\nnetwork:\n  # Does the model consider the expansion of sub-region to sub-region transmission\n  # capacity\n  transmission_expansion: True\n  # Does the model consider the expansion of renewable energy zone transmission\n  # capacity\n  rez_transmission_expansion: True\n  # Years to annuitise transmission project capital costs over.\n  annuitisation_lifetime: 30\n  nodes:\n    # The regional granularity of the nodes in the modelled network (only sub_regions is implemented)\n    regional_granularity: sub_regions\n    # Whether Renewable Energy Zones (REZs) are modelled as distinct nodes\n    # Only discrete_nodes implemented\n    rezs: discrete_nodes\n  # Line capacity limit for rez to node connections that have their limit's modelled\n  # through custom constraint (MW).\n  rez_to_sub_region_transmission_default_limit: 1e5\n\n\n# ===== Temporal =======================================================================\n\ntemporal:\n  year_type: fy\n  range:\n    # Model begin at the start of the start year. E.g. the first time interval for a\n    # financial year model starting in 2025 would be 2024-07-01 00:30:00.\n    start_year: 2050\n    # Model ends at the end of the start year. E.g. the last time interval for a\n    # financial year model ending in 2028 would be 2028-06-01 23:30:00.\n    end_year: 2050\n  capacity_expansion:\n    resolution_min: 30\n    reference_year_cycle: [2018]\n    # List of investment period start years. An investment period runs from the beginning\n    # of the year (financial or calendar depending on the config) until the next the\n    # period begins.\n    investment_periods: [2050]\n    aggregation:\n      # Representative weeks to use instead of full yearly temporal representation.\n      # Options:\n      #   ~ (None): Full yearly temporal representation is used or another aggregation.\n      #   list[int]: a list of integers specifying weeks of year to use as representative. Weeks of year are defined as\n      #   full weeks (Monday-Sunday) falling within the year. For example, if the list is \"[1]\" the model will only use the\n      #   first full week of each modelled year.\n\n      representative_weeks: ~\n\n      # Named representative weeks based on demand characteristics\n      #   ~ (None): Full yearly temporal representation is used or another aggregation.\n      #   list[str]: peak-demand, residual-peak-demand, minimum-demand,\n      #     residual-minimum-demand, peak-consumption, residual-peak-consumption\n      named_representative_weeks: [residual-peak-demand]\n\n  operational:\n    resolution_min: 30\n    reference_year_cycle: [2018]\n    horizon: 336\n    overlap: 48\n    aggregation:\n      # Representative weeks to use instead of full yearly temporal representation.\n      # Options:\n      #    ~ (None): Full yearly temporal representation is used or another aggregation.\n      #   list[int]: a list of integers specifying weeks of year to use as representative. Weeks of year are defined as\n      #   full weeks (Monday-Sunday) falling within the year. For example, if the list is \"[1]\" the model will only use the\n      #   first full week of each modelled year.\n      representative_weeks: ~\n\n      # Named representative weeks based on demand characteristics\n      #   ~ (None): Full yearly temporal representation is used or another aggregation.\n      #   list[str]: peak-demand, residual-peak-demand, minimum-demand,\n      #     residual-minimum-demand, peak-consumption, residual-peak-consumption\n      named_representative_weeks: [residual-peak-demand, peak-consumption, residual-minimum-demand]\n\n\n# ===== Solver =======================================================================\n\n# External solver to use\nsolver: highs\n\n\n# ===== Plotting =====================================================================\ncreate_plots: True\n</code></pre>"},{"location":"tables/ispypsa/","title":"ISPyPSA input tables","text":"<p>Coming soon</p>"},{"location":"tables/ispypsa/#custom_constraint_lhs","title":"custom_constraint_lhs","text":""},{"location":"tables/ispypsa/#custom_constraints_rhs","title":"custom_constraints_rhs","text":""},{"location":"tables/ispypsa/#flow_paths","title":"flow_paths","text":""},{"location":"tables/ispypsa/#flow_path_expansion_costs","title":"flow_path_expansion_costs","text":""},{"location":"tables/ispypsa/#renewable_energy_zone","title":"renewable_energy_zone","text":""},{"location":"tables/ispypsa/#rez_transmission_expansion_costs","title":"rez_transmission_expansion_costs","text":""},{"location":"tables/ispypsa/#sub_regions","title":"sub_regions","text":""},{"location":"tables/pypsa_friendly/","title":"PyPSA friendly Tables","text":"<p>Coming soon</p>"}]}